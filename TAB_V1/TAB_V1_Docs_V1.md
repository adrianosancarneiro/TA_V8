# Team Agent Builder (TAB) – Module Documentation

## Overview

The **Team Agent Builder (TAB)** is the module responsible for guiding users through the creation and configuration of multi-agent teams, domain knowledge bases, and tenant environments within the broader Team Agent Platform. It serves as the “agent team factory” – providing both an interactive, conversational wizard and a direct configuration import pathway to define new AI agent teams. In the context of the Team Agent Platform, which includes the Team Agent Orchestrator (TAO) for runtime workflow coordination and the Team Agent Executor (TAE) for executing agent tasks, TAB operates at the design phase. It ensures that agent teams are defined with all necessary roles, tools, and knowledge before they are orchestrated by TAO and run by TAE. This module lowers the barrier to building complex AI agent teams by offering a high-level guided interface, making multi-agent orchestration accessible even to non-developers.

**Key Responsibilities:** TAB provides two primary modes for creating an agent team or related configuration: (1) a **Conversational Wizard** (AI-guided, using natural language interactions) to walk users through each decision, and (2) a **YAML Config Importer** for users who want to supply a full or partial configuration in YAML format directly. In both cases, TAB validates inputs, suggests improvements (such as relevant tools or even pre-built sub-team integrations), and produces a finalized configuration that is stored in the system’s database and ready for deployment. TAB is also multi-tenant aware, meaning it isolates configurations per organization (tenant) and enforces role-based permissions on who can create or modify agent teams.

At a high level, the TAB module plays a pivotal role in the Team Agent Platform’s lifecycle: a user (with appropriate permissions) initiates a new build for an AI agent team (or domain knowledge base or tenant onboarding) via TAB; TAB then either parses an input YAML or engages in a guided Q\&A with the user to gather requirements. Once the specification is complete and validated, TAB persists the new **Team Definition** (and related Domain or Tenant settings if applicable) to the database and notifies the rest of the platform (particularly TAO) that a new agent team is available. TAO can then orchestrate these agents in collaborative workflows, and TAE can execute each agent’s actions as defined. In essence, **TAB is the design-time companion to TAO’s runtime orchestration**, ensuring that what gets orchestrated is well-defined and aligned with user intent.

## Architectural Overview

The architecture of TAB is organized into several submodules, each handling a distinct aspect of the build process. The design emphasizes modularity and reusability – each sub-component of TAB can function independently (with well-defined inputs/outputs), providing ROI as standalone tools, while collectively they work in concert to deliver the full team-building workflow. The major components of TAB include:

* **WizardEngine:** A conversational, AI-driven wizard that interacts with the user to collect team configuration requirements. This is built on LangGraph (a structured agent orchestration framework) to manage the dialogue flow with durability and control[\[1\]](https://blog.langchain.com/building-langgraph/#:~:text=%2A%20Human,the%20only%20way%20to%20go). The WizardEngine is used not only for building agent teams but also adapted for **domain knowledge configuration** and **tenant onboarding**, leveraging a similar question-and-answer flow for each use case.

* **YAMLValidator & Importer:** A parser and validator for YAML configuration files. This allows users to import a pre-written team/tenant/domain spec in YAML. The submodule checks the file for correctness and completeness. If the YAML config is valid and complete, it bypasses the interactive flow and proceeds to final creation; if incomplete or invalid, it gracefully hands off to the WizardEngine to interactively resolve any gaps or errors.

* **TeamBuilderWorkflow Coordinator:** The orchestration logic that manages the overall build session. It coordinates between the wizard and YAML import paths, maintains session state, and handles transitions (e.g. from YAML import to wizard if needed). It is responsible for creating a **Build Session** record, tracking progress (via steps), and finalizing the output (persisting it and triggering integration points). This workflow component ensures that building a team is a structured, multi-step process rather than a single request, enabling pause/resume and user-in-the-loop validation at each step.

* **Semantic Tool Recommender:** A utility that uses semantic vector search (via a Qdrant vector database) to suggest relevant tools or even existing agent teams (as composite tools) that match the user’s needs. During the build process, when the user describes an agent’s goal or a domain’s topic, this component finds potentially useful tools from the platform’s tool registry by meaning, not just exact name matching. It leverages Qdrant’s ability to retrieve contextually relevant information using hybrid semantic+lexical search[\[2\]](https://qdrant.tech/ai-agents/#:~:text=Contextual%20Precision%20Qdrant%E2%80%99s%20hybrid%20search,based%20strategies). This can significantly speed up configuration by recommending proven solutions for common requirements.

* **Session Manager (Build Sessions & Steps):** An internal subsystem (often part of the workflow coordinator) that handles persistence and retrieval of in-progress build sessions. It uses a database (PostgreSQL for relational data, and Neo4j for graph relationships where applicable) to store the state of a user’s current configuration progress. This allows a user to pause the configuration process and resume later exactly where they left off, even if the underlying process was interrupted or the system restarted. Thanks to the LangGraph-based design, the wizard can truly checkpoint its state and resume after long delays without keeping an in-memory process alive[\[1\]](https://blog.langchain.com/building-langgraph/#:~:text=%2A%20Human,the%20only%20way%20to%20go). The Session Manager also logs each interaction step (questions asked and answers given) for auditability and debugging.

* **Integration Interfaces:** These are not single modules but a set of touchpoints where TAB connects with other parts of the platform. For example, an **Orchestrator API Client** or database triggers might notify the TAO module when a new team configuration is finalized, so that TAO can load or register the new team. Similarly, integration hooks ensure that when a new tenant or domain is created via TAB, the relevant records propagate to systems that need them (like access control lists, default domain knowledge indexing jobs, etc.).

Internally, TAB is implemented as a service (or set of services) that expose a well-defined API (detailed in a later section) to the front-end or other callers. It is typically deployed as a container (Docker-based), and it may be scaled horizontally if multiple concurrent build sessions need to be supported. The diagram below (conceptually) shows the flow:

* **User** (via UI or API) initiates a build \-\> **TAB API** (TeamBuilderWorkflow) creates a session and either launches WizardEngine or calls YAMLValidator \-\> user and WizardEngine engage in a loop of Q\&A (with Semantic Tool suggestions as needed) or YAML errors are corrected \-\> once all data is collected, TeamBuilderWorkflow finalizes the configuration \-\> data is saved to **Postgres/Neo4j** (team/tenant/domain records) and **TAO/TAE** are informed of the new configuration.

*(In the actual implementation, the above might be subdivided further, but this provides a high-level architectural context.)* The subsequent sections break down each submodule in detail, covering purpose, design, and usage.

## WizardEngine – Conversational Build Wizard

**Purpose & Overview:** The WizardEngine provides an interactive, conversational interface for building agent teams, configuring domain knowledge, or onboarding a new tenant. It leverages LLM technology to conduct a dialogue with the user, asking questions about the desired team or configuration in a natural language format. Under the hood, WizardEngine uses the LangGraph framework to structure this dialogue as a graph of steps/nodes, ensuring the conversation follows a logical flow and can be paused or resumed reliably[\[1\]](https://blog.langchain.com/building-langgraph/#:~:text=%2A%20Human,the%20only%20way%20to%20go). The wizard’s goal is to gather all necessary information to fully specify an agent team (or domain/tenant) – for example, the team’s overall objective, the number and roles of agents, tools each agent should use, any domain-specific knowledge or context to include, and configuration metadata like names or descriptions.

**ROI as Standalone:** As a component, the WizardEngine can be reused for any complex configuration task that benefits from a guided Q\&A approach. It could be repurposed as a general **Conversational Form** engine outside of just agent teams – e.g., onboarding users through complex setup, or interactive troubleshooting. Its ability to incorporate AI for understanding user inputs and to handle “human-in-the-loop” interactions means it could drive other chat-driven workflows where the system needs structured output from a flexible dialogue. This separation of the wizard logic makes it valuable as a standalone module in other contexts.

**How it Works (Process Flow):** The wizard operates as a state machine of dialogue steps: 1\. **Initialization:** When triggered for a new build session, the WizardEngine loads a predefined template of questions or a conversation graph tailored to the task (team creation, domain config, or tenant setup). It creates an initial state (possibly empty config structure) and determines the first question to ask (e.g., “What is the primary goal of the agent team?” for team creation). 2\. **Dynamic Questioning:** The engine asks the user one question at a time via the front-end interface. These questions are dynamically generated or selected based on prior answers. For example, after asking for the team’s goal, the wizard might ask how many agents or what roles are needed. If the user says “I need an agent to handle web research and another to summarize findings,” the WizardEngine can parse that answer (using the LLM) to identify two roles (“Web Researcher” and “Summarizer”) and then proceed to ask follow-up questions about each agent (e.g., what tools should the Web Researcher use?). The LangGraph framework ensures the sequence of steps can branch conditionally or loop as needed, while keeping track of state. 3\. **Semantic Suggestions:** At certain points, the wizard can invoke the Semantic Tool Recommender. For instance, when discussing tools for a “Web Researcher” agent, the wizard might call the recommender with a query like “web search tool” or based on the agent’s role description. The recommender returns a list of tool suggestions (e.g., a WebSearch API tool, a Browser tool, etc.), which the wizard presents to the user: “Here are some tools that might help with web research: WebSearchTool, WebScraperTool, etc. Would you like to include any of these?” The user can accept suggestions or specify other tools. This dramatically improves the user experience by leveraging existing knowledge within the platform. 4\. **Validation at Each Step:** User inputs are validated in-line. If the user provides an answer that is unclear or out-of-bounds, the wizard detects it (potentially using both rule-based checks and the LLM’s interpretation). For example, if a user names a tool that is not recognized, the wizard can respond, “I’m not familiar with that tool. Could you describe what it does, or would you like to pick from a list of known tools?” Similarly, if a required piece of information is missing or contradictory, the wizard will ask for clarification. The integration with YAML validation logic means the wizard essentially ensures that by the end of the dialogue, all fields required by the final config schema are collected and consistent. 5\. **Checkpointing & Pause/Resume:** After each question-answer exchange, the WizardEngine updates the **Build Session** state (persisting the partial configuration and the next expected question). This enables the user to pause at any time. Thanks to LangGraph’s design, the agent (wizard) doesn’t need to remain in memory waiting; it can serialize its state and effectively “interrupt” itself until new input arrives[\[1\]](https://blog.langchain.com/building-langgraph/#:~:text=%2A%20Human,the%20only%20way%20to%20go). When the user returns, the wizard session is looked up and the conversation resumes at the correct point, possibly after reloading state from the database. This durability ensures even long-running configuration processes (which might span hours or days if the user takes time to think or gather info) are supported without issues. 6\. **Completion:** Once the wizard has gathered all necessary info, it typically summarizes or echoes the configuration to the user for confirmation. For example: “Alright, I have the following configuration for your team: Team Name X, with 2 agents (A doing Y with tools T1, T2; B doing Z with tool T3), operating under domain D. Does that look correct?” The user can approve or request changes. After final confirmation, the wizard’s role ends and it signals the TeamBuilderWorkflow to finalize the build (persist and trigger integration).

**Internal Models & Helper Classes:** The WizardEngine is implemented using a combination of **LangGraph nodes** and Python classes representing steps. Each step in the wizard might correspond to a function or method (for example: ask\_team\_goal(), ask\_agent\_roles(), ask\_tools\_for\_agent(agent), confirm\_configuration(), etc.). These could be modeled as separate nodes in a LangGraph StateGraph, allowing non-linear transitions and parallelism where appropriate. The wizard likely uses helper classes such as: \- BuildContext: an object representing the partial configuration collected so far (with attributes for team name, goal, list of agents, etc.). \- WizardPromptTemplates: a set of prompt templates for the LLM to phrase questions or interpret answers. (E.g., a template to ask for tools might be “What tools or APIs should the {agent\_role} agent use to accomplish its tasks?”). \- AnswerParser: logic (possibly using regex or even LLM functions) to parse free-form user input into the structured fields needed. For instance, if the user answers a multi-part question in a paragraph, this parser helps extract the pieces. \- SessionStorage: an interface to the session manager for saving/loading state.

Because the WizardEngine is AI-driven, it also includes safeguards: e.g., limiting the length of questions, avoiding sensitive data in prompts, and ensuring the conversation stays on topic. The tone of the wizard can be configured (friendly vs. strictly formal) as needed.

**Integration Touchpoints:** The WizardEngine interacts with several other parts of TAB: \- It calls the **Semantic Tool Recommender** API whenever it needs to fetch suggestions (this is typically an internal call within the module, possibly just a function call to a submodule with the query text). \- It uses the **Session Manager** to save progress after each user input. For example, after the user provides agent details, the WizardEngine will update the build\_steps log and commit the new partial config to the build\_sessions record. \- It may reference the **YAML Importer** in cases where the wizard was launched as a follow-up to an incomplete YAML. In that scenario, the WizardEngine might be pre-loaded with certain fields (from the parsed YAML) and will intelligently skip those questions, focusing only on missing parts. The user will experience a seamless flow (the wizard might say “I see you provided some configuration file. I’ll ask about any details that were missing or unclear.”). \- When the wizard is for **Domain Knowledge** or **Tenant Config**, it might integrate with external systems: e.g., for domain configuration, if the user needs to attach knowledge sources, the wizard might eventually call out to a knowledge ingestion pipeline. Or for tenant creation, after collecting details, it might trigger an email to the new tenant admin. However, those actions are coordinated by the workflow after wizard completion; the wizard’s main integration is confined to data collection and validation.

**Implementation Structure:** In the code layout, WizardEngine likely resides in a package like tab/wizard/. There might be separate classes or scripts for each type of wizard: \- TeamBuildWizard – orchestrating Q\&A for agent team creation. \- DomainConfigWizard – for domain knowledge base setup (e.g., asking for domain name, description, data sources). \- TenantOnboardingWizard – for new tenant configuration (e.g., company name, admin user info, default settings).

These could subclass a common BaseWizardEngine that provides generic functionality (like session persistence hooks, ability to send prompts to the LLM, etc.). The wizard can be invoked via the workflow controller whenever a session of the appropriate type is started.

To illustrate, a snippet of how the wizard might be initiated in code:

wizard \= TeamBuildWizard(session\_id=..., user\_id=..., tenant\_id=...)  
wizard.start()  \# This would load the first question

Then as user responses come in:

wizard.process\_answer(user\_input)  
\# behind the scenes: this logs the answer in build\_steps, updates partial config,  
\# then determines the next question (or triggers completion if done).  
next\_question \= wizard.get\_next\_prompt()

This loop continues until wizard.is\_complete() returns true.

**Concrete Test Plans:** Testing the WizardEngine involves both unit testing of the conversation logic and integration testing with the full API: \- *Unit Tests:* One can simulate a sequence of answers programmatically. For example, feed a predetermined set of answers for a known scenario (like building a 2-agent team) and assert that the final BuildContext produced by the wizard matches the expected configuration. Also test edge cases: if an answer is invalid, verify that the wizard asks again or provides an error prompt. If the wizard is given a partial context (as in YAML fallback), check that it only asks for missing fields. \- *AI Behavior Tests:* Because an LLM is in the loop, it’s important to test with the LLM in a deterministic way if possible. This might involve using a fixed prompt/response pair (perhaps by injecting a fake LLM that returns canned responses). For example, ensure that when a user says “I’m not sure what tools I need,” the wizard, via the LLM prompt, responds with a helpful clarification or triggers the recommender. \- *Pause/Resume:* Simulate a pause by stopping after a few steps, storing the session, then reloading it (perhaps in a new wizard instance) and confirming that it continues correctly. For instance, user answers 2 questions, we simulate the user leaving – the state should be saved. Then create a new wizard instance with the same session ID and call resume(), it should prompt with question 3 next. \- *Multi-type flows:* Test that the wizard can handle the domain config and tenant config flows. Each of those might skip certain team-specific questions. For example, domain config wizard shouldn’t ask about agents at all; instead it might ask about data sources or knowledge file upload. Verify that it follows the correct path and ends up creating a domain entry in the DB. \- *Concurrent Sessions:* Ensure that two different wizard sessions (for two different users or two different teams) do not interfere. This can be tested by running two wizards in parallel threads or async tasks with different session IDs and ensuring their states remain separate. \- *Performance:* While not typically a unit test, it’s good to verify that the wizard doesn’t become sluggish with many steps. Given that each step involves a DB write and possibly an LLM call, integration tests can include timing checks (e.g., the system should handle, say, 10 concurrent sessions without excessive latency).

In summary, the WizardEngine is the interactive heart of TAB, providing a user-friendly yet powerful way to configure AI agent teams by having a conversation rather than writing code. Its design leverages advanced agent orchestration concepts (LangGraph for structure, human-in-loop interruptions[\[1\]](https://blog.langchain.com/building-langgraph/#:~:text=%2A%20Human,the%20only%20way%20to%20go), etc.) to ensure reliability and flexibility.

## YAMLValidator & Importer – Configuration File Path

**Purpose:** The YAML Validator and Importer submodule allows users to define an entire team (or domain/tenant configuration) via a YAML file and then ingest that file into the system. This caters to advanced users or automated pipelines where a textual configuration is preferred. The purpose is twofold: to speed up the build process by accepting configurations in bulk, and to maintain consistency by using a declarative approach. Many AI orchestration frameworks support YAML or JSON definitions for multi-agent setups[\[3\]](https://medium.com/the-savvy-canary/avoiding-multi-agent-system-complexity-with-yaml-f4f958610930#:~:text=The%20YAML%20way), and TAB follows this best practice by providing a standardized YAML schema for agent teams. By offering an import route, TAB also enables **version control** and repeatability – configurations can be stored in a repo, shared, and re-used across environments.

**ROI as Standalone:** As a component, the YAML import/validation logic could be used outside the interactive context. For example, the TAO module (or an offline batch process) could utilize the YAML parser to load team definitions from files at startup. Or, if the platform offers a library of pre-designed teams, those might be stored as YAML and loaded via this module. The validation part is particularly reusable – it’s essentially a schema enforcement tool. It could be applied to ensure any config (even ones built via the wizard) adhere to the expected format. As a standalone service, one could imagine a “Config Linter” service that checks AI agent configuration files for correctness (similar to how CI pipelines lint config files).

**How it Works:** The YAML import workflow proceeds as follows: 1\. **User Submits a YAML:** The user either uploads a YAML file through the UI or sends it via an API call (e.g., a POST /build\_sessions with a YAML content attached or a reference to it). The YAML is expected to describe the agent team or other entity in a structured way, following the platform’s specification. An example snippet of a team YAML might be:

team\_name: "MarketResearchTeam"  
description: "A team of agents that gather and summarize market research data."  
agents:  
  \- name: "WebResearcher"  
    role: "Finds information on the web"  
    tools: \["WebSearchTool", "WebScraperTool"\]  
  \- name: "Summarizer"  
    role: "Aggregates and summarizes info"  
    tools: \["TextSummarizerTool"\]  
    model: "gpt-4"  
domain: "Market Analysis"

This example defines a team with two agents, their roles, assigned tools, and associates the team with a domain. The actual schema may allow more fields (like agent types, prompts, or connections between agents), but this illustrates the general idea. 2\. **Parsing and Schema Validation:** Once the YAML text is received, the YAMLValidator component attempts to parse it. Under the hood, it likely uses a YAML parsing library (e.g., PyYAML or similar) to convert the text into a Python dictionary/object. Then it validates this object against the expected schema. The validation can include: \- **Structure checks:** Required top-level sections (e.g., agents list must be present for a team; if it’s a domain config YAML, maybe knowledge\_sources must be present, etc.), correct data types (e.g., name is a string, tools is a list of strings), and allowed value ranges (e.g., no more than N agents maybe, or known tool names if we enforce that here). \- **Semantic checks:** Cross-field validation such as each agent name should be unique, each tool listed should exist in the tool registry (or at least be recognized; this might also be deferred to wizard suggestions if not recognized), domain names might need to conform to certain patterns or refer to an existing tenant, etc. The validator might be implemented via a predefined JSON Schema or using Pydantic models for strict parsing. If the YAML does not conform, the module will produce specific error messages indicating what is wrong (e.g., “Field team\_name is missing” or “Tool XYZ not found in tool registry”). 3\. **Complete vs. Partial Config:** If the YAML passes validation and includes **all required information**, TAB can proceed to create the configuration immediately without further user input. In this case, the TeamBuilderWorkflow will take the parsed config object and skip to finalization (persisting it and notifying other systems). On the other hand, if the YAML is **partial or has errors**: \- The workflow will initiate a **fallback to the WizardEngine**. It will create a build session and load the partial data into the wizard’s context. The wizard then only asks about missing pieces or clarifications. For example, if the YAML defined the agents and tools but left out the team’s overall description or domain, the wizard will start by saying, “I’ve loaded your configuration. I need a bit more information to finish setting up your team. What is the overall goal or description of this team?” Then continue as normal. If there were errors (like an unknown tool), the wizard can address those: “The tool XYZ in your config wasn’t recognized. Could you provide details or choose an alternative?” This collaborative approach ensures that users who attempt the YAML import aren’t left with a cryptic error – they get a chance to fix it interactively. \- The YAMLValidator would have to communicate to the WizardEngine which parts are missing or invalid. This could be done by annotating the parsed object with error flags or by generating a list of “issues” that the wizard treats as a to-do list of questions. 4\. **Security considerations:** The YAML import is carefully handled to avoid any code injection or malicious content. YAML is a potentially dangerous format if loaded naively (due to YAML references, anchors, etc.). The importer likely uses safe load options (no exec of tags) and does not allow arbitrary Python objects. It treats the YAML purely as data. Also, any references in YAML (like file paths or URLs in a domain config) are validated to ensure they point to allowed locations. 5\. **After Import – Unified Path:** Once the YAML data is either fully validated or completed via wizard, the flow converges with the normal wizard path – meaning at the end we have a completed configuration that can be saved.

**Internal Models & Implementation:** The YAMLValidator might be structured as follows: \- A schema definition (possibly a JSON Schema file or a set of Pydantic models). For example, a Pydantic model AgentConfig with fields: name: str, role: str, tools: List\[str\], model: Optional\[str\], etc., and a TeamConfig model with team\_name, description, agents: List\[AgentConfig\>, domain, .... If using Pydantic, simply loading the YAML via these models will automatically perform type checking and can provide error messages for missing fields. \- A validation function that post-processes the object for any interdependencies (like ensuring tools exist by checking against a list from the database or ensuring the referenced domain exists for the given tenant). \- An importer function that takes either a file or text input and returns either a fully validated config object or a partial object plus a list of issues.

If the YAML includes certain complex structures (like agent prompt templates or chain definitions), those are either stored as is or validated to conform to allowed patterns (to avoid, for example, excessive token length or unsafe instructions). In MVP, the YAML schema is likely constrained to basic fields.

**Integration Touchpoints:** \- If the YAML references existing entities (like “tenant: X” or “domain: Y”), the validator will query the database to verify those exist. This is an integration with the persistence layer or possibly with TAO if TAO manages those records. Typically, TAB can directly query the Postgres DB for tenants/domains since it has access. \- The YAMLImporter uses the Workflow Coordinator to either proceed with creation or spawn a wizard session. That means after validation, it hands control (and data) to either TeamBuilderWorkflow.finalize\_from\_config() or to TeamBuilderWorkflow.launch\_wizard\_with\_partial(config). \- No direct LLM integration here (unlike wizard), except that the wizard might come into play later. However, one potential integration: If the YAML is in an older format or missing recommended fields, the system might use an LLM to guess or suggest fixes (this would be an advanced feature beyond MVP – e.g., “Your YAML didn’t specify agent roles clearly; I’ve guessed some based on agent names.”). We note this as a possible future enhancement in roadmap, but not main flow now. \- On successful import (no wizard needed), integration with TAO/TAE is same as normal: likely through the finalize step writing to DB or sending an event.

**Implementation Structure:** The code for this likely resides in tab/importer/ or similar. It could contain: \- schema/ folder with YAML schema definitions. \- yaml\_validator.py containing functions to validate given a YAML string. \- exceptions.py defining errors like InvalidConfigError that includes details. \- If using an external library for schema, that config might be here as well.

**Example (Pseudo-code):**

from schema import TeamConfigSchema  \# e.g., a Pydantic model

def import\_yaml(yaml\_text: str):  
    try:  
        data \= yaml.safe\_load(yaml\_text)  
    except Exception as e:  
        raise InvalidConfigError(f"YAML parsing failed: {e}")  
    \# Validate structure:  
    try:  
        config \= TeamConfigSchema(\*\*data)  
    except ValidationError as e:  
        issues \= parse\_validation\_errors(e)  \# convert to user-friendly messages  
        raise InvalidConfigError(f"Schema validation failed", issues=issues)  
    \# Additional semantic checks:  
    issues \= \[\]  
    for agent in config.agents:  
        if not tool\_exists(agent.tools):  
            issues.append(f"Unknown tool: {tool}")  
    ...  
    if issues:  
        raise InvalidConfigError("Semantic validation failed", issues=issues)  
    return config  \# fully validated config object

The workflow calling this function would catch InvalidConfigError. If issues exist but the config object was partially constructed, it may pass that partial data to wizard. If the YAML couldn’t be parsed at all, it might directly respond to the user with an error rather than invoking wizard (since a completely broken YAML might need fixing offline).

**Concrete Test Plans:** \- *Full YAML Success:* Supply a well-formed YAML defining a simple team. Expect the function to return a config object with correct fields, and the system should create the team entry in the DB without any wizard interaction. Test that all fields persist correctly (team name, each agent, etc.). Also test that TAO can see this team (if TAO reads from the same DB, verifying integration). \- *YAML with Minor Omissions:* Remove a non-critical field from YAML (e.g., description). The validator should flag it, the workflow should invoke wizard. Test that the wizard indeed asks for the missing description and after answering, the team is created correctly with that description. \- *YAML with Errors:* Examples: \- Unknown tool name: The validator finds a tool that isn’t in the known list. We expect it to not outright fail but rather to go to wizard for resolution. The wizard should ask something like “Tool X was not recognized, what is it for or do you want to use a different tool?” If the user then chooses a known tool, the process continues. Verify the final config replaces or includes the resolved tool properly. \- Missing agent roles or mis-structured data: e.g., an agent entry with no name. This is required, so the YAMLValidator likely fails schema validation. For such a fundamental error, it might not make sense to go into wizard (because how do we identify that agent without a name?). The system might simply return a clear error for the user to fix in YAML manually. The test here is that the error message is understandable (“Agent entry 1 is missing a name”) and that no partial session is left open in this case. \- *Security test:* Try a YAML that includes something weird like an \!Ref or other YAML advanced feature to ensure the parser doesn’t execute those. The outcome should be either a safe parse ignoring those or an error. Also, test extremely large YAML (size limits) to ensure the system handles or rejects appropriately (to prevent denial-of-service via huge files). \- *Domain and Tenant YAML:* If supported, test importing a domain config via YAML (e.g., containing domain name and list of knowledge source URLs). Also tenant via YAML (maybe name, admin email, etc.). Each should either directly create those entries or prompt wizard for missing bits. For example, a domain YAML might list a data file but not provide content; the wizard might then ask the user to upload that file.

In summary, the YAML Importer offers a faster path for those who prefer to *write* configurations rather than *talk through* them. It’s fully integrated with the wizard as a fallback, ensuring no matter how incomplete the input is, the system can guide the user to completion. This dual approach (manual YAML or conversational wizard) reflects a design that accommodates both power users and casual users.

## TeamBuilderWorkflow – Session Orchestration and Finalization

**Purpose:** The TeamBuilderWorkflow acts as the conductor that ties together the various components (WizardEngine, YAMLImporter, etc.) into a coherent process. Its primary responsibility is to manage a **build session** from start to finish, ensuring that the user’s request to create a team/tenant/domain goes through the necessary steps, and that resources are cleaned up or finalized properly. Essentially, this is the high-level state machine for the entire build lifecycle. It abstracts whether the user is using the wizard or a direct import – from the perspective of external callers (like the front-end or API client), they just interact with “a build session” and the workflow takes care of routing to wizard questions or handling direct completion.

**ROI as Standalone:** As a workflow orchestration component, this could be generalized to manage any multi-step guided process in the platform. For example, if in the future there are other processes like “Agent Team Deployment” (with steps like select environment, schedule downtime, confirm deploy), a similar workflow engine could handle that. The TeamBuilderWorkflow’s structure of session tracking, step logging, and branching could be repurposed for other orchestrated interactions beyond team building. It provides a pattern for building long-running, user-interactive tasks in a reliable way (with logging, rollback, etc.), which has value as a general utility.

**How it Works:** The workflow can be described in phases: \- **Session Creation:** When a user triggers a new build (via an API call like POST /build\_sessions with parameters), the workflow initializes a new **BuildSession** record in the database. This includes assigning a unique session ID, recording who started it (user ID, and their tenant context), what type of build it is (team, domain, or tenant), and a status (e.g., “in\_progress”). The session may also record if it started from YAML or wizard. \- **Dispatch to Wizard or Importer:** The workflow looks at the request data: \- If a YAML file/content is provided, it invokes the YAMLValidator on that input. Depending on the outcome, it either pre-populates session data and sets a flag that wizard is needed, or it marks the session as ready to finalize (if YAML was complete). \- If no YAML is provided, it immediately starts the WizardEngine for that session. \- In practice, even if YAML is provided, if it’s incomplete the workflow will still start the wizard. It might instantiate the WizardEngine with the partial config loaded. \- **Session Loop (Wizard-driven):** If the wizard is engaged, the workflow essentially waits for inputs to come in from the user. Each time the user submits an answer (via an API endpoint, e.g., POST /build\_sessions/{session\_id}/messages or similar), the TeamBuilderWorkflow passes that to the WizardEngine instance for processing. The wizard returns either the next question or indicates completion. The workflow then returns the next prompt to the front-end (which shows it to the user). This loop continues, with the workflow managing the state transitions: \- It logs each step to the build\_steps table (question asked, answer given, timestamp). \- It keeps the build\_sessions record updated (e.g., perhaps storing the last step number or a blob of the current partial configuration). \- It monitors for special commands like if the user requests a pause or cancel. \- **Pause & Resume Handling:** If a pause is requested (or user simply disconnects), the workflow doesn’t terminate the session; it leaves it in “paused” state. To resume, the user would call a resume endpoint or simply request the next question later. The workflow will then reload the session context (which includes the wizard state via LangGraph’s checkpoint) and instruct the wizard to continue from where it left off. This mechanism heavily relies on the persistent state—no user input is ever lost; everything is journaled so that resuming is straightforward. \- **Completion & Finalization:** Once the WizardEngine indicates that all data is collected (or the YAML path was complete from the start), the workflow transitions the session to a “completed” state. In this phase: \- It calls any final validation (for example, run the YAMLValidator one more time on the assembled config to ensure consistency – though if wizard did its job, this is mostly redundant). \- It **persists the final artifacts**: This could mean creating entries in various tables. For an agent team, this might involve creating records for the team, for each agent, linking tools, etc., possibly in both Postgres and Neo4j (more on the schema in the next section). For a domain, it creates the domain entry and associates with tenant. For a tenant, it creates the tenant record and default settings. \- It triggers **post-build actions**: Integration with TAO/TAE. Likely, the simplest integration is that TAO periodically or on-demand reads from the database the list of agent teams. If using Neo4j, TAO might use the graph representation. In MVP, perhaps TAB simply ensures the data is available and maybe sends a signal. A possible implementation: after finalization, the workflow could publish an event on a message queue like “TEAM.CREATED” with the team ID, so TAO can pick it up and load the new team into its orchestrator if needed. Or TAO might simply always query when needed. In any case, the workflow’s responsibility is to ensure the new configuration is **registered** and accessible. \- It might also notify the user or system: e.g., return the final summary to the user (“Team X has been successfully created\!”). If via API, the final response could include identifiers for the created entities (team\_id, etc.). \- **Cleanup:** Optionally, the workflow could clean up session data (like remove the partial config from the session record, or mark it as archived) once finalization is done. However, often keeping the history is useful for audits. So likely the session record remains with status “completed” and references to the outputs.

Throughout this process, the TeamBuilderWorkflow ensures **consistency**: e.g., if an error occurs mid-way (like a server crash or a dependency failure), the session can be resumed without data loss. If a fatal error occurs during finalization (say DB transaction fails), the workflow can roll back or mark the session as “error” state, to be retried.

**Internal Models & Helper Classes:** \- The core model here is the **BuildSession** (which we’ll detail in the data schema section). The workflow uses this model to store status and also possibly to lock a session when in progress (to avoid two concurrent modifications). \- A **WorkflowController** or similar class might encapsulate these steps. For instance, a method start\_session(user, type, yaml\_file=None) kicks off a session. Another submit\_answer(session\_id, answer) drives the wizard one step and returns the next question (and updates DB). A finalize(session\_id) does the closing steps (though finalize might be automatically called when wizard signals completion). \- It likely uses the Session Manager component for persistence operations. The Session Manager might expose methods like save\_step(session\_id, step\_data) and get\_session(session\_id) etc., which the workflow calls at appropriate times. \- **Concurrency:** If the system is deployed as multiple instances, there is a design question: can two different service instances handle the same session at different times (e.g., before and after a pause)? If so, session state must truly come from the database (and LangGraph checkpoint) rather than memory. The workflow likely ensures that – when resuming, it doesn’t rely on in-memory objects being present (which they wouldn’t if a different instance handles it). Instead it rehydrates the WizardEngine state from stored checkpoints. This implies the WizardEngine either serializes its entire state (LangGraph can serialize node states and channels to MsgPack or JSON[\[4\]](https://blog.langchain.com/building-langgraph/#:~:text=want%20to%20save%20checkpoints%20that,user%20or%20developer%20for%20input)[\[1\]](https://blog.langchain.com/building-langgraph/#:~:text=%2A%20Human,the%20only%20way%20to%20go)) into the database or an object store. \- The workflow might also implement timeouts or expiration: for example, if a session is left idle for too long (say 7 days), it might auto-close or alert an admin. MVP might not include this, but it’s a consideration.

**Integration Touchpoints:** \- **TAO Integration:** As mentioned, finalization is where integration happens. In MVP, integration could be as simple as populating the shared database so that TAO can access the new definitions. For example, TAO might have a component that reads all agent teams and their agent/tool setups from Neo4j (where they are stored as graphs). In that case, TAB’s finalization would include writing the team graph to Neo4j. If TAO instead uses Postgres, then writing to the teams and related tables suffices. Because the user specifically asked for integration points, we highlight: *TAB does not directly run or orchestrate agents* – that’s TAO’s job. But TAB must ensure TAO knows about new teams. In a microservice architecture, possibilities include: \- Direct DB sharing (TAO reads from the same DB tables teams, agents, tools). \- REST API call (TAB calls an endpoint on TAO like /orchestrator/register\_team with the team config data). \- Message/event (TAB emits an event consumed by TAO).

MVP might rely on the simplest: shared DB and maybe a manual refresh trigger. Future might incorporate event-driven updates. \- **TAE Integration:** TAE (Team Agent Execution) is likely the part that actually holds the runtime for agents (loading models, running agent loops). For TAE to execute a team, it needs the config too. Possibly TAO passes the config to TAE when starting a task, or TAE also has DB access. In terms of TAB’s role, ensuring the config (like agent roles, prompt templates, tool selections) is stored in the canonical form is the integration. If any pre-processing is needed (like registering new tools with the tool execution engine), TAB might call that. For instance, if the user added a custom tool in wizard (e.g., a new API key or a new integration), TAB might need to update the Tools repository or notify some service to make that tool available. This again depends on architecture. We can say: integration wise, when new tools or external connections are defined in the team, appropriate records or credentials are saved such that TAE can use them. \- **Permissions check:** The workflow likely also integrates with security. Before starting or finalizing, it may verify the user has rights to do so (though the API layer might already do that). For example, if user tries to create a team under a tenant they don’t own, it should reject. Or only an admin can create a new tenant – the workflow would enforce that rule by checking user roles at session start. \- **Audit logging:** It might send logs or metrics about the build process (like how long it took, success/failure). Integration with monitoring: e.g., update a Prometheus counter for “teams\_created\_total”.

**Implementation Structure:** The TeamBuilderWorkflow might be implemented within the API controller itself or as a service class that the API calls. Likely, there is a module like tab.core.workflow that contains functions or a class to encapsulate the flow. The API endpoints (see later section) will mostly be thin wrappers that call into this workflow logic. For example:

\# Pseudo-code for endpoint handling:  
def start\_build(request):  
    user \= request.user  
    payload \= request.json or request.form  
    yaml\_file \= payload.get('yaml')  
    build\_type \= payload.get('type')  \# "TEAM" / "DOMAIN" / "TENANT"  
    session \= WorkflowController.start\_session(user, build\_type, yaml\_file)  
    return {"session\_id": session.id, "first\_question": session.current\_prompt}

And:

def answer\_build(request, session\_id):  
    user \= request.user  
    answer \= request.json.get('answer')  
    next\_prompt \= WorkflowController.submit\_answer(session\_id, user, answer)  
    return {"next\_prompt": next\_prompt, "completed": next\_prompt is None}

(This is illustrative; actual implementation might differ especially with streaming or chunked answers.)

**Concrete Test Plans:** \- *End-to-End Happy Path:* Simulate via API calls a full team creation: call start (with or without YAML) \-\> then loop sending answers \-\> finalize. At the end, verify that the database contains the new team properly. Also verify the response sequence to the client is as expected (the questions make sense, final result is delivered). \- *Pause/Resume Workflow:* Start a session, get a couple of steps in, then simulate a pause (could be as simple as not calling next for a bit, or explicitly hitting a pause endpoint if available). Then after a delay, call resume (or just call next with the same session). Ensure that the workflow picks up correctly. Under the hood, you’d test that the build\_sessions record retained the state and that a new instance of the workflow can use it. Possibly in a unit test, instantiate two workflow objects to mimic two processes. \- *Concurrent Builds:* Start two sessions for the same user (should that be allowed?) or for different users. Ensure no cross-talk or data leak between them. The build\_sessions table keys and handling should isolate them. Also test that if two sessions try to write at once (maybe unlikely unless user double clicks start), one is handled properly (maybe reject a second concurrent session for the same user/type to avoid confusion). \- *Error Handling:* Intentionally trigger an error mid-workflow. For example, cause the database to be unavailable right when finalizing (one could mock the DB call to throw). The workflow should handle it gracefully – ideally marking the session as “error” and not leaving partial entries. In a test environment, one can simulate DB failure and see if the system rolls back (e.g., using a transaction). The user should get a clear error response. After an error, see if resume is possible or if they must restart – possibly the session could be retried after error. \- *Integration Points:* If an event or API call to TAO is supposed to happen at finalization, write a test stub for that. E.g., monkeypatch a TAO client method and verify it was called with correct data when a build completes. \- *Domain/Tenant Workflows:* Test these specifically. For a domain config, the workflow might slightly differ (for instance, after domain creation, maybe trigger a background job to index documents – ensure that trigger is called). For tenant, after creation, maybe ensure the new tenant is accessible (like one could attempt to create a team under that tenant as next step).

The TeamBuilderWorkflow is essentially the backbone that ensures all the parts work together in sequence. It provides reliability (with session logs and error handling) and a clean interface for outside interaction. It abstracts the complexity of whether the build is conversational or file-driven. By following this orchestrated approach, the platform ensures a consistent outcome regardless of input method.

## Semantic Tool & Team Recommendation (Qdrant Integration)

**Purpose:** The semantic recommendation component is focused on enhancing the build process with intelligent suggestions. The idea is to leverage existing knowledge about tools and agent capabilities to help the user build a better team faster. When a user describes an agent’s role or the problem domain, this component finds relevant tools (or even whole pre-built teams) that could be directly incorporated or referenced. For example, if the user is creating a data analysis agent, the system might suggest “SpreadsheetAnalyzerTool” if it exists, instead of the user needing to know or remember it. Likewise, if an entire team already exists to handle a sub-problem, the system might suggest using that as a subsystem (the “team-as-tool” concept).

**ROI as Standalone:** As a separate service, this recommender can act as a general semantic search API for the platform. It could be exposed in a “Tool Marketplace” where users query for tools. It can also be useful within TAO at runtime – agents could query the same index to find if a tool or agent is available to solve a subtask. Essentially, it’s a vector search capability over the platform’s knowledge base of tools and solutions. This is a classic example of reusing infrastructure: the Qdrant vector database can store embeddings of any text; here we store descriptions of tools and team capabilities. The search mechanism itself could be reused for document retrieval or other semantic search needs in the platform. The module’s value stands in enabling **discoverability** of resources via natural language.

**How it Works:** \- **Data Indexing:** First, all available tools and possibly agent teams are indexed. This likely happens offline or periodically. For each tool, we have a textual description (e.g., “WebSearchTool: allows an agent to search the web for information using Google’s API”). These descriptions are converted to embeddings (vectors) using an embedding model (like OpenAI’s text-embedding models or similar). The vectors are stored in Qdrant, with metadata such as the tool’s ID, name, and perhaps tags. Similarly, if we treat existing teams as recommendable, each team could have a synopsis (e.g., “FinancialAnalysisTeam: a group of agents that collaboratively analyze financial reports”). Those are also embedded and stored, possibly in the same Qdrant collection or a separate one labeled differently. \- **Query Embedding and Search:** During a build session, when appropriate (usually when the user has provided some description of what they need), the WizardEngine or workflow calls this recommender. For instance, the user might say the agent’s role is “summarize text documents” – the system can take that phrase and create an embedding for it using the same embedding model. Then it queries Qdrant for nearest vectors (the hybrid search in Qdrant can combine semantic similarity with optional filters[\[2\]](https://qdrant.tech/ai-agents/#:~:text=Contextual%20Precision%20Qdrant%E2%80%99s%20hybrid%20search,based%20strategies)). The result is a list of potentially relevant items, each with a similarity score. \- **Suggestion Filtering and Ranking:** The raw results from Qdrant might be filtered by context. For example, only suggest **tools** for which the user’s tenant has access (if some tools are premium or restricted, filter by metadata like tenant\_allowed:true). Or if recommending teams-as-tools, maybe only suggest ones marked as “public” or belonging to the same tenant (to avoid suggesting something the user cannot actually use due to permissions). The remaining results are sorted by score (and possibly by item type priority – maybe tools first, teams second, depending on UI). \- **Presenting Suggestions:** The wizard then presents these suggestions in the conversation. It might say, “I recommend considering the following tools for this agent: ToolA (does XYZ) and ToolB (does ABC)[\[5\]](https://qdrant.tech/ai-agents/#:~:text=Contextual%20Precision%20Qdrant%E2%80%99s%20hybrid%20search,Hybrid%20Search). Would you like to add any of these to the agent’s toolkit?” The descriptions can come directly from the stored metadata. The user can accept (e.g., “Yes, include ToolA”) or decline (“No, I think I need something else”). If the user is unsure, they could ask for more info, and possibly the wizard could provide that (maybe by retrieving extended description or documentation link if stored). \- **Team-as-Tool Mechanism:** If an entire team is suggested (let’s call it TeamT), using it as a tool means instead of building an agent for that function, the new team will call TeamT via TAO as a subroutine. Implementation-wise, that could mean the config for the new team includes a special agent that acts as a proxy to TeamT. Or simpler: the wizard might just inform the user that “there’s already a team that does X, you might integrate with it or use its output.” In MVP, team suggestions might be more for awareness; full “team plugging into team” might be a future expansion. Still, we include the concept in design: the platform envisions agents (or teams) that can call other agent teams as if they were tools, enabling hierarchical problem solving. The recommender would identify such opportunities. \- **Integration with Workflow:** The workflow uses the recommender synchronously during the wizard flow. Outside of that, the recommender likely doesn’t run. It might also be exposed via an endpoint if the front-end wanted to query it directly (for instance, a sidebar “search tools” feature). But primarily it’s invoked by the wizard automatically when needed. \- **Technologies:** Qdrant is the chosen vector DB because it allows semantic similarity search at scale. It also supports filtering by metadata (which we use for multitenancy and type filters) and is optimized for performance in this scenario[\[6\]](https://qdrant.tech/ai-agents/#:~:text=Multi,Multitenancy). The embedding model could be the same LLM backend or a specialized model (like SentenceTransformers). Given this is platform internals, it likely uses a precomputed embedding library for efficiency.

**Internal Models:** \- The tool registry (likely a table of tools with descriptions) works in tandem. There might be a routine to sync that data to Qdrant when tools are added or updated. \- The Qdrant client is used for querying. Possibly encapsulated in a SemanticSearchService class with methods like search\_tools(query\_text, top\_k=5) \-\> List\[ToolRecommendation\]. \- ToolRecommendation might be a simple data class containing tool\_id, name, description, similarity\_score. \- If team suggestions are included, those might reuse the same class or a similar one TeamRecommendation with team\_id, name, description, etc. \- **Semantic caching:** We might not implement it in MVP, but Qdrant itself supports some hybrid search and might cache results. If multiple similar queries occur, it could help. But not critical to document.

**Integration Touchpoints:** \- **Database:** Source of truth for tool data. The recommender must reflect what's actually available. If a tool requires an API key or setup, it should only be suggested if it’s usable. Possibly the tool records have flags like enabled=True, or even per-tenant availability. The recommender query likely includes the current tenant context (to filter). \- **Wizard UI:** The integration with the UI is important. The suggestions need to be displayed in a user-friendly way. If the UI is chat-based, it will just show as part of the assistant’s message. If the UI has a sidebar or modal for suggestions, the backend might provide structured data. Possibly, the API returns suggestions in a structured form along with the wizard’s message. The documentation might mention an endpoint like /suggestions?query=... but if it's internal, we can mention that suggestions are included in wizard responses. \- **Permission/Security:** Ensure that no suggestion reveals something user shouldn’t see. E.g., if there’s a tool that is secret or not yet released, it should not be suggested. This is handled by filtering with metadata in Qdrant (only index items the user can access or mark them and filter). \- **Performance:** Integration with Qdrant means another service call. We assume Qdrant is either deployed as part of the platform or offered via API (with appropriate API key). The environment variables for Qdrant connection (like QDRANT\_URL, QDRANT\_API\_KEY) will be needed and are part of config (will mention in deployment section). \- **Example:** Suppose the user says “I need the agent to search the web”. The system queries Qdrant with “search the web”. Qdrant likely returns something like \[ (“WebSearchTool”, score=0.95), (“WebBrowserAgentTeam”, score=0.90) \]. The wizard might present: “Suggested tool: **WebSearchTool** – which allows web queries[\[5\]](https://qdrant.tech/ai-agents/#:~:text=Contextual%20Precision%20Qdrant%E2%80%99s%20hybrid%20search,Hybrid%20Search). Also, an existing agent team **WebBrowserAgent** can retrieve and summarize web pages. You can use the tool for direct searches or integrate the agent team for complex browsing.” The user then might pick one.

**Test Plans:** \- *Embedding/Index Consistency:* Add a new tool in the tool registry and ensure it gets indexed properly. Then query using keywords in that tool’s description to see if it’s returned. This may involve a unit test where we call the indexing routine, then the search function with a relevant query. \- *Basic Suggestion Flow:* In a simulated wizard session, after certain input, call the recommender. For example, input “The agent should translate text from English to French.” If the platform has a “TranslationTool”, ensure it appears in suggestions. The test would assert that the recommended tool list contains the expected item and the descriptions are correct. \- *Ranking:* If multiple tools match, ensure the most relevant one comes first. This could be tested by having distinct tool descriptions and a query that matches one strongly. \- *Filtering:* Mark a tool as unavailable for a given tenant, then run a query as that tenant – ensure that tool is not in results. Conversely, for team suggestions, perhaps mark one team as public and one as private (owned by another tenant), ensure only public appears. \- *Edge Cases:* Query that yields no result – the recommender should handle gracefully (maybe return an empty list, and the wizard would then not mention suggestions in that turn). Test that a completely unknown query (like “flibbertigibbet”) yields no suggestions and doesn’t break anything. \- *Performance:* Feed a bunch of queries in quick succession to simulate many suggestions (maybe 10 requests concurrently) and see that it responds within acceptable time (Qdrant is quite fast, but network latency and embedding generation time matter). If using an external embedding API, that could be slow; possibly we use a local model for embeddings to speed up. \- *Accuracy:* While subjective, if possible, test that for some typical roles the suggestions make sense (this could be part of QA rather than automated test, by reviewing top suggestions for common roles/domains).

By integrating semantic search, TAB moves beyond a static form-filling approach to a smarter assistant that leverages the platform’s collective knowledge. This helps users discover capabilities they might not think of, ensuring more powerful and well-equipped agent teams. The use of Qdrant’s vector search ensures these recommendations are contextually relevant[\[2\]](https://qdrant.tech/ai-agents/#:~:text=Contextual%20Precision%20Qdrant%E2%80%99s%20hybrid%20search,based%20strategies), combining semantic understanding with precise retrieval.

## Domain and Tenant Configuration Flows

In addition to building agent teams, TAB supports two related configuration flows: **Domain Knowledge Base configuration** and **Tenant configuration**. These use the same underlying mechanisms (wizard or YAML) as agent team building, but are worth describing separately because they focus on different types of data and have different implications in the platform.

### Domain Knowledge Configuration

**Purpose:** A *Domain* in this context refers to a knowledge domain – essentially a collection of information or context that agents might need to know about. For example, a domain could be “Acme Corp Policies” containing documents and Q\&A that agents at Acme Corp should be aware of, or “Financial Market Data” for agents dealing with finance. Configuring a domain typically involves specifying what the domain is and what knowledge sources to include (documents, databases, websites, etc.). TAB’s role is to allow creation of these domain configs so that the knowledge can be integrated into agent teams or agent responses.

**Wizard Flow for Domain:** When a user (likely a tenant admin or knowledge manager role) initiates a domain creation, the WizardEngine adapts its questions accordingly: \- It may ask for the **Domain Name** and a brief **Description** (“What is the domain or knowledge base you want to create, and how would you describe it?”). \- It asks about **Knowledge Sources**: for instance, “Do you want to import documents or connect a data source for this domain’s knowledge?” The user might be given options: upload files (PDFs, text), enter key information directly, or link an external source (like a SharePoint, a database, etc.). In MVP, it might simply handle uploading text or providing a list of documents. \- It handles **Processing Options**: If there are large documents, the wizard might ask if they should be chunked or if any summarization is needed. (This could also be automatic or deferred to an ingestion pipeline.) \- It might also ask about **Domain Scope**: e.g., which agent teams should use this domain by default, or is it a global domain? This could tie into TAO – some orchestrators tag agents with domain knowledge contexts.

Under the hood, much of the wizard logic is reused. For example, missing info will be prompted similarly. The final output of a domain config flow is an entry in the domains table with references to stored knowledge (the actual documents might be stored in an S3 bucket or a vector DB – beyond TAB’s direct scope, but TAB initiates it).

**YAML for Domain:** A YAML could also be used to define a domain:

domain\_name: "AcmePolicies"  
description: "HR and company policies for Acme Corp."  
tenant: "AcmeCorp"    \# reference to tenant  
sources:  
  \- type: "pdf"  
    path: "s3://acme-knowledge/policies.pdf"  
  \- type: "website"  
    url: "https://acme.example.com/employee-handbook"

TAB’s YAMLImporter would validate such a file, ensure the tenant exists and maybe check the path formats. If sources are provided, it might not immediately ingest them (that might be a separate process, say a knowledge ingestion service), but it stores the references so that ingestion can happen.

**Integration:** Once a domain is configured, integration points include: \- Notifying a **Knowledge Indexer** service to actually process the documents (if applicable). This might happen asynchronously. The TAB module might simply mark the domain as pending indexing. \- The domain is linked to tenant: ensure multi-tenant isolation (one tenant’s domain isn’t accessible to another). \- TAO might incorporate domain context by ensuring that when agents from a given domain run, they have access to that info. Possibly TAO or TAE will use the domain ID to fetch relevant knowledge (e.g., query a vector store or database for context). \- Domain config often overlaps with vector DB usage as well (embedding the documents for retrieval by agents). While not done by TAB itself, TAB’s output triggers those processes.

**Security & Permissions:** Typically, only users with elevated permissions can create or modify domain knowledge configurations, because it could involve uploading sensitive data. So TAB enforces that (described more in security section) – e.g., a “Tenant Knowledge Manager” or admin role is required.

**Testing Domain Flow:** We would test that providing some example documents leads to a domain entry and that any subsequent process (perhaps simulated) picks up those sources. Also test partial flows – if user doesn’t have a document ready, maybe the wizard allows finishing the domain config and adding documents later.

### Tenant Configuration (Onboarding)

**Purpose:** Tenant configuration refers to setting up a new tenant (organization or client) on the platform. This is typically a process only done by a platform admin (because creating a new tenant is akin to provisioning a new space in the multi-tenant system). The information collected includes the tenant’s name, possibly logo or branding details, default settings (like default language or model preferences), and the initial admin users. The Tenant configuration flow in TAB aims to streamline the onboarding of a new organization into the platform, ensuring all necessary data is captured so that the tenant environment is ready for use (with correct database entries, default domain possibly, etc.).

**Wizard Flow for Tenant:** If a platform admin uses TAB to create a tenant: \- It asks for **Organization Name** (tenant name) and maybe an **Organization ID** or slug (for URLs or unique identification). \- It may ask for a **Description** or industry, etc., to tailor some defaults. \- It will ask for **Admin User Details**: Typically, the primary admin’s email and name, since that might be needed to create the user account that can manage that tenant. (It might even trigger sending an invitation email to that admin – integration point.) \- **Default Domain Knowledge:** Possibly, the wizard could ask if they want to immediately set up a domain knowledge base (like company policies as domain). In MVP, this could be skipped or minimal. \- **Settings:** This could include what LLM models are allowed or default (if the platform allows per-tenant model settings), or enabling/disabling certain features. For example, some tenants might not want internet access for agents due to policy – the wizard might surface these as checkboxes or questions (“Should agents in this tenant have web access? Yes/No”). MVP might keep this simple, but placeholders for such config exist. \- **Confirmation:** Summarize the tenant profile and confirm creation.

**YAML for Tenant:** Possibly a YAML could define a tenant too:

tenant\_name: "Globex Inc."  
admin\_user:  
  email: "admin@globex.com"  
  name: "John Doe"  
settings:  
  allow\_internet: false  
  default\_domain: "GlobexKnowledge"

The YAMLImporter would validate that (ensuring email is valid, etc.) and proceed similarly.

**Integration:** Creating a tenant has system-wide implications: \- **Database:** A new entry in tenants table is created with a unique ID. This might cascade create separate schema or partitions if the system isolates tenant data at DB level (some systems use separate schemas or even separate DBs per tenant – but given we have a tenants table, likely shared schema with tenant\_id as foreign key on others). \- **Defaults:** Possibly create a default domain or default agent team (some platforms create a sample agent or domain for new tenants). If so, TAB might trigger those as sub-tasks or instruct the admin how to do it next. \- **User Provisioning:** The admin user details may be handed off to an identity management service. TAB might call an Auth service API to create that user and assign them the tenant admin role. Or it might simply record the intent, and an external process picks it up. \- **Notifications:** The platform might send a welcome email or such. Not directly TAB’s responsibility to send, but it might call a service or produce an event.

**Security:** Only a super-admin can run this flow. That’s enforced at the API layer and double-checked in the workflow (if somehow called by non-admin, it should error out).

**Testing Tenant Flow:** Ensure that after running it, the tenant table has the entry, the admin user is created (maybe stub out the user creation call in tests). Also test trying to run it as non-admin is forbidden.

**Reuse of Mechanisms:** It’s worth noting that from an implementation standpoint, domain and tenant flows reuse the same *Session*, *WizardEngine*, *Workflow* infrastructure. The differences lie in the question set and the final data writing. The TeamBuilderWorkflow likely has a parameter for build\_type which branches to different question flows and different finalization logic:

if session.type \== 'TENANT':  
    \# call TenantWizard and finalize by creating tenant record  
elif session.type \== 'DOMAIN':  
    \# call DomainWizard and finalize by creating domain record  
else:  
    \# default to team wizard and finalize by creating team

But they all share logging, pause/resume, etc. This avoids duplicating the entire system for each entity type.

In summary, **Domain and Tenant configurations** are first-class citizens in TAB, handled with the same care as agent teams. This means an organization can not only build their agent teams but also set up the context (knowledge and organizational boundaries) using one unified interface. The consistency of using the wizard across these ensures a uniform user experience – whether you’re creating a new team or setting up a knowledge base, you interact in a conversational, guided manner.

## Data Schema (Postgres & Neo4j) for Build and Configuration Records

The TAB module interacts with persistent storage to save both the process state (sessions, steps) and the resultant configurations (teams, domains, tenants). We describe here the key tables in PostgreSQL (and analogous node/relationship structures in Neo4j if applicable) that are directly relevant to the team/tenant/domain build process. These schema definitions ensure that data is organized and accessible for both TAB’s needs and the wider platform integration.

### Build Session and Steps (Process Tracking)

**build\_sessions Table:** This table tracks each ongoing or completed build session (a “session” corresponds to one run of the wizard/import process for a specific outcome). Key columns might include: \- session\_id (UUID, Primary Key): Unique identifier for the session. \- type (VARCHAR or ENUM): The type of build, e.g. 'TEAM', 'DOMAIN', or 'TENANT'. This lets the system know which flow and which final tables to target. \- status (VARCHAR or ENUM): e.g. 'in\_progress', 'paused', 'completed', 'error'. Indicates the current state. \- user\_id (UUID, Foreign Key to users table): The user who initiated the session. \- tenant\_id (UUID, Foreign Key to tenants table): Context – for team or domain builds, the tenant under which this is happening. (For tenant creation itself, this might be null or self-referential at the end). \- started\_at (TIMESTAMP), updated\_at (TIMESTAMP): Timestamps for audit. \- partial\_config (JSONB): This could store the partial configuration as it’s being built. For example, after the user has answered some questions, this JSON might contain whatever details have been collected so far (team name, some agents, etc.). This is helpful for resuming the wizard and also for debugging if something goes wrong. \- current\_step (INT or VARCHAR): Optionally, track the last step number or an identifier for the next question node in the wizard graph. (This, along with partial\_config, can be used to restore the exact point in the conversation.) \- yaml\_imported (BOOLEAN): flag if this session began with a YAML import. If true, partial\_config may be pre-filled from YAML.

In Neo4j: likely no direct representation of a build session, since that’s more of a transient process. Build sessions are probably only in Postgres (for transactionality and ease of queries).

**build\_steps Table:** This table logs each interaction step within a session (essentially a transcript and action log). Columns: \- step\_id (SERIAL or UUID PK): Unique ID for the step log entry. \- session\_id (UUID, FK to build\_sessions): The session this step belongs to. \- step\_number (INT): An incremental counter of steps in that session (1, 2, 3, ...). \- prompt (TEXT): The question or prompt that was presented to the user (could also include system messages from wizard). \- response (TEXT): The user’s answer/input for that step. If the user uploaded a file or YAML at start, that could be recorded as a special entry here as well (or in build\_sessions). \- timestamp (TIMESTAMP): When this interaction occurred. \- notes (TEXT, optional): This could include any metadata like “wizard auto-suggestion offered” or “validation error encountered and corrected”. Not mandatory, but useful for debugging/tracing.

This step log provides an audit trail and can be used for training or analyzing how people use the wizard (for continuous improvement). It also can be useful if a session errors out – you can see what the last step was, etc.

**Relationships:** build\_steps obviously has a many-to-one relationship to build\_sessions. If using an ORM, one can get session with its list of steps.

Example schema definitions (simplified SQL-like form):

CREATE TABLE build\_sessions (  
    session\_id UUID PRIMARY KEY,  
    type TEXT CHECK (type IN ('TEAM','DOMAIN','TENANT')),  
    status TEXT CHECK (status IN ('in\_progress','paused','completed','error')),  
    user\_id UUID REFERENCES users(user\_id),  
    tenant\_id UUID REFERENCES tenants(tenant\_id),  
    started\_at TIMESTAMPTZ DEFAULT NOW(),  
    updated\_at TIMESTAMPTZ DEFAULT NOW(),  
    partial\_config JSONB  
);  
CREATE TABLE build\_steps (  
    step\_id SERIAL PRIMARY KEY,  
    session\_id UUID REFERENCES build\_sessions(session\_id),  
    step\_number INT,  
    prompt TEXT,  
    response TEXT,  
    timestamp TIMESTAMPTZ DEFAULT NOW()  
);

We would also have an index on session\_id in build\_steps for quick retrieval.

### Tenant and Domain Records (Configuration Data)

**tenants Table:** This table stores tenant (organization) information. It likely exists outside of TAB too (since login and other modules use it), but TAB writes to it when a new tenant is onboarded. Key columns: \- tenant\_id (UUID PK): Unique tenant identifier. \- name (TEXT): Tenant name. \- description (TEXT): Possibly a description or notes. \- created\_at (TIMESTAMP), created\_by (UUID referencing users): Audit info for creation. \- settings (JSONB): A blob for various tenant-specific settings (like allowed tools, default LLM model, etc., as configured during onboarding or later in an admin panel). \- status (TEXT): e.g., 'active', 'inactive'. (If a tenant can be disabled.) \- Other fields: maybe logo URL, contact email, etc., but those are beyond our focus.

In Neo4j: If modeling in graph, a Tenant could be a node labeled Tenant with properties name, etc. Agents or domains might have relationships to Tenant node (like :BELONGS\_TO).

**domains Table:** Stores domain knowledge configs. Columns: \- domain\_id (UUID PK). \- tenant\_id (UUID FK to tenants): The tenant that this domain belongs to. \- name (TEXT): Domain name. \- description (TEXT). \- created\_at, created\_by similar to above. \- Possibly status or indexed (BOOLEAN) to indicate if knowledge indexing is done. \- config (JSONB): Details of the domain configuration. This might list source references, any specific parameters (like embedding model used, etc.). Alternatively, there could be a separate table for domain sources if we need to normalize it, but a JSON list of sources might suffice in MVP.

In Neo4j: Domain could be a node label as well (e.g., Domain node, with relationship to Tenant). If agent teams link to domains, that might be represented as relationships in graph (e.g., TEAM node connected to a Domain node with relation USES\_DOMAIN).

**Relationships:** One tenant can have many domains (one-to-many). Also one tenant can have many teams (though the teams might be stored differently; if there's a teams table or nodes). Domain likely belongs to exactly one tenant in our design. If there’s a concept of global domain (shared across tenants) that’s probably avoided for security; each is isolated.

Example:

CREATE TABLE tenants (  
    tenant\_id UUID PRIMARY KEY,  
    name TEXT,  
    description TEXT,  
    created\_at TIMESTAMPTZ,  
    created\_by UUID REFERENCES users(user\_id),  
    settings JSONB  
);  
CREATE TABLE domains (  
    domain\_id UUID PRIMARY KEY,  
    tenant\_id UUID REFERENCES tenants(tenant\_id),  
    name TEXT,  
    description TEXT,  
    created\_at TIMESTAMPTZ,  
    created\_by UUID,  
    config JSONB  
);

(We assume users table exists with user\_id PK for references.)

### Agent Team Configuration Data

While not explicitly asked, it’s worth noting how agent team definitions might be stored once built, since that’s the outcome of TAB: \- There might be a teams table or similar. Often, complex structures like multi-agent teams are stored in multiple tables (teams, agents, tools mapping, etc.) or in a graph DB. \- Given Neo4j is mentioned, one likely approach is to store agent team composition in Neo4j. For example, when a team is finalized, the system creates a node for the Team, nodes for each Agent, nodes for each Tool, and relationships such as (Team)-\[:HAS\_AGENT\]-\>(Agent), (Agent)-\[:USES\_TOOL\]-\>(Tool). Neo4j is well-suited to querying relationships, like finding which tools a team uses, etc. TAO could then quickly traverse this graph to orchestrate communication or dependencies between agents. \- If Postgres is used for team structure too (maybe a simpler approach for MVP): There could be: \- teams table (team\_id, name, tenant\_id, domain\_id optional, etc.). \- agents table (agent\_id, team\_id, name, role, model, etc.). \- agent\_tools table (agent\_id, tool\_name or tool\_id references a tools table). \- tools table (tool\_id, name, description, etc.), which might be pre-populated with known tools (and perhaps augmented if user defines new custom tool – that’s maybe future feature).

However, since the prompt said only include schemas for build sessions, steps, tenants, domains, we won’t detail teams and agents tables. But the documentation can note that final agent team configs are persisted, possibly in graph form.

**Note on Neo4j Usage:** If Neo4j is used: \- A *Tenant* node might connect to *AgentTeam* nodes it owns. \- An *AgentTeam* node connects to *Agent* nodes and *Domain* node (if domain attached). \- *Agent* nodes connect to *Tool* nodes. \- Possibly *Agent* nodes of one team could connect to other *AgentTeam* nodes if using team-as-tool concept (like a relation “CALLS\_TEAM” or similar). \- All these nodes would have properties corresponding to what you’d find in a table (like name, description, etc.).

**Test the Schema:** As part of development, we would ensure that: \- Creating a new tenant via TAB indeed inserts a row in tenants. \- Similarly for domain. \- On team creation, all relevant records/nodes are created (though we won’t deep-dive here). \- The build\_sessions and build\_steps correctly record the process without data loss or inconsistency. For example, ensure that when a session completes, you can reconstruct what happened from build\_steps and the final config from the combination of partial\_config (which might hold final config at end) or from the actual created team entry. \- Foreign keys: e.g., if a tenant is deleted (not a common operation), domain and team records should handle that (cascade or restrict). Usually, you wouldn’t delete tenants often; if needed, maybe cascade delete their stuff. \- Indices: verify that querying by tenant\_id or session\_id is efficient.

In documentation, highlighting the **traceability**: build\_steps provides trace, and linking build\_sessions to the final outcome (maybe store the new team\_id or domain\_id in build\_sessions upon completion as well, for reference).

By defining the above schema, the platform ensures all essential information is stored in a structured way: \- Process data (sessions/steps) for resilience and analysis. \- Configuration data (tenant/domain and implicitly team/agent) for the runtime usage.

These structures are central to how TAB communicates with TAO/TAE: the latter will read from tenants and domains (and likely teams/agents) to know what exists. It’s the contract between the builder and the orchestrator.

## Deployment and Operational Considerations

The Team Agent Builder module is designed to be deployed as a containerized service, with configuration through environment variables, standard health checks for orchestration, and metrics for observability. This section describes how to deploy TAB (especially via Docker), what environment variables and configurations it requires, and how it supports health monitoring and metrics collection (e.g., for Prometheus).

**Docker Deployment:** TAB is packaged as a Docker image (e.g., team-agent-builder:latest). The image encapsulates the TAB application, which is likely a Python (FastAPI or Flask) service given the context, along with any necessary dependencies (LangChain/LangGraph, Qdrant client, database drivers, etc.). To deploy: \- Ensure that Postgres, Neo4j, and Qdrant services (if self-hosted Qdrant) are accessible to the container (via network). \- Launch the TAB container with the required environment variables (detailed next). \- Typically, the container will run a web server (like Uvicorn for FastAPI) on a certain port (say 8000\) to serve API requests. When deploying in Kubernetes or Docker Compose, you’d map this to an appropriate external port or service.

**Environment Variables:** TAB uses environment variables for configuration, making it flexible across environments (dev, staging, prod). Key variables include: \- **Database Connection:** e.g., POSTGRES\_HOST, POSTGRES\_PORT, POSTGRES\_DB, POSTGRES\_USER, POSTGRES\_PASSWORD (or a single DATABASE\_URL that encapsulates these). These allow TAB to connect to the Postgres instance for storing sessions and config data. \- **Neo4j Connection:** if Neo4j is used, variables like NEO4J\_URI (e.g., bolt://neo4j:7687), NEO4J\_USER, NEO4J\_PASSWORD would be needed for the builder to write team graphs. If the MVP doesn’t directly write to Neo4j (maybe TAO does it), this may not be present in TAB. \- **Qdrant Connection:** QDRANT\_URL (the endpoint of the Qdrant service, e.g., http://qdrant:6333 if in Docker network) and possibly QDRANT\_API\_KEY if Qdrant is secured. These are used by the Semantic Tool Recommender to query the vector DB[\[6\]](https://qdrant.tech/ai-agents/#:~:text=Multi,Multitenancy). \- **LLM API Keys:** If the WizardEngine uses an external LLM (like OpenAI API), environment vars such as OPENAI\_API\_KEY or others (Azure keys, etc.) would be required so that the wizard’s LLM calls are authenticated. Similarly, if using other AI services for embeddings or processing, their keys must be provided (e.g., OPENAI\_EMBEDDINGS\_KEY if separate, or HuggingFace model identifiers). \- **Service Config Flags:** e.g., TAB\_DEBUG (to enable debug logging), TAB\_LOG\_LEVEL, or flags like ENABLE\_SUGGESTIONS=true/false if one wants to toggle the semantic suggestions (perhaps to disable in low-resource environments). \- **Port and Host:** Typically framework defaults are fine, but one might allow override like PORT for the web server port. \- **Prometheus and Metrics:** Usually, no special env vars needed if using default /metrics path, but you might have ENABLE\_METRICS flag or configure metric labels via env. For example, something like METRICS\_NAMESPACE=tab to prefix metric names.

**Health Checks:** The TAB service exposes one or more health endpoints to allow orchestrators (like Kubernetes) to verify that it’s running properly. Common patterns: \- **Liveness Probe** – e.g., an endpoint /health/live that returns a simple status (HTTP 200 OK with maybe “alive” message) to indicate the app is running. This might not check dependencies, just that the process is up. \- **Readiness Probe** – e.g., /health/ready that checks critical dependencies (database connection, etc.) and returns 200 if everything is in order. The Kubernetes configuration would hit this endpoint to decide if the service can receive traffic. If, say, the database is unreachable at startup, the ready check would fail, and Kubernetes won’t send traffic yet (preventing errors). \- A general combined health endpoint (like /health/ returning a JSON status of various components) can also be present[\[7\]](https://dev.to/lisan_al_gaib/building-a-health-check-microservice-with-fastapi-26jo#:~:text=What%20This%20Project%20Covers). For example:

{  
  "status": "ok",  
  "database": "ok",  
  "vector\_db": "ok",  
  "llm\_api": "ok"  
}

If any of those checks fail, status might be “degraded” or HTTP 500 returned.

Internally, implementing these might involve trying a trivial DB query (or using a library’s health check feature) and pinging Qdrant’s API (Qdrant has a /health endpoint as well) within the request handler[\[7\]](https://dev.to/lisan_al_gaib/building-a-health-check-microservice-with-fastapi-26jo#:~:text=What%20This%20Project%20Covers). The results are aggregated. The endpoint should be quick to respond so as not to bog down health checks.

**Prometheus Metrics:** TAB is instrumented to expose metrics in Prometheus format on an endpoint like /metrics. This is common in web services and often comes almost for free with frameworks or a simple integration[\[8\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=following%3A%20First%2C%20let%E2%80%99s%20see%20http%3A%2F%2Flocalhost%3A5000%2F,query%20bar%20and%20running%20Execute). For instance, if using FastAPI, one might use prometheus\_fastapi\_instrumentator to automatically track HTTP requests, response times, etc. Out of the box metrics could include: \- HTTP request count and latency (http\_requests\_total, http\_request\_duration\_seconds with labels for endpoint, method, status)[\[9\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=from%20prometheus_client%20import%20Counter)[\[10\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=REQUEST_COUNT%20%3D%20Counter%28%20,). \- Memory usage, Python GC stats (if enabled via client library). \- Custom metrics specific to TAB: \- tab\_build\_sessions\_total{status} counting how many build sessions started/completed/failed. \- tab\_recommendations\_served\_total counting how many tool suggestions were given. \- tab\_build\_duration\_seconds histogram measuring how long it takes to complete a build session. \- tab\_yaml\_import\_count vs tab\_wizard\_used\_count to see usage of each path.

These metrics allow operators to monitor the usage and performance of TAB. For example, an increase in failed build sessions could trigger an alert. The /metrics endpoint is scraped by Prometheus server periodically (like every 15s or 30s)[\[8\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=following%3A%20First%2C%20let%E2%80%99s%20see%20http%3A%2F%2Flocalhost%3A5000%2F,query%20bar%20and%20running%20Execute). We ensure not to put anything sensitive in metrics.

**Logging:** (Not explicitly asked, but notable for operations). The service likely logs events in structured form (JSON logs or key info) to stdout, which can be captured by Docker or cluster logging. Logging includes startup info, each API call, warnings if any config validation fails, etc. In production, debug logs might be off to reduce noise.

**Scaling and Availability:** In a containerized deployment, one can run multiple instances of TAB behind a load balancer if needed. Because TAB maintains state in the database and uses stateless API calls, it should scale horizontally. However, one consideration: do not run multiple instances if using an in-memory wizard state without proper shared storage. But as we discussed, wizard uses DB and checkpointing for state, so multiple instances should be fine – a user’s session can be handled by any instance as long as they always fetch the latest state from DB. A sticky session (same user to same instance) is not strictly necessary unless some in-memory cache is used for performance (not in MVP likely).

**Docker Compose Example:** A snippet might look like:

services:  
  tab:  
    image: team-agent-builder:latest  
    environment:  
      \- POSTGRES\_HOST=db  
      \- POSTGRES\_USER=tab\_user  
      \- POSTGRES\_PASSWORD=...   
      \- POSTGRES\_DB=team\_agent  
      \- QDRANT\_URL=http://vector\_db:6333  
      \- OPENAI\_API\_KEY=${OPENAI\_API\_KEY}  
      \- TAB\_LOG\_LEVEL=INFO  
    ports:  
      \- "8000:8000"  
    depends\_on:  
      \- db  
      \- vector\_db

This ensures the builder waits for the database and vector DB. In K8s, similar environment config via ConfigMap/Secret.

**Health and Metrics in K8s:** \- We would configure readinessProbe to call /health/ready and livenessProbe /health/live for example, with some initial delay and period. The builder code will return quick "OK" or error accordingly. \- Prometheus scraping: if using the Prom operator or similar, we annotate the deployment to scrape port 8000/metrics. Then in Grafana or any dashboard, we could visualize those metrics (like number of active sessions, etc.). For example, checking the count of http\_requests\_total can confirm the service is being used and not stuck[\[8\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=following%3A%20First%2C%20let%E2%80%99s%20see%20http%3A%2F%2Flocalhost%3A5000%2F,query%20bar%20and%20running%20Execute).

**Security in Deployment:** Ensure that environment variables (like DB passwords, API keys) are passed securely (in Kubernetes, use Secrets). The metrics endpoint does not expose sensitive info; if concerned, one can put it behind auth or at least not expose it publicly. The health endpoints might also be protected (or at least contain minimal info) because sometimes detailed health check can leak version or dependency info – but typically showing “db: ok” is fine.

**Updates and Migration:** Deploying new versions of TAB might require database migrations (e.g., adding a new field to build\_steps). It’s important to use migration tools (like Alembic for SQL) during deployment to ensure the schema is updated. Because of microservice architecture, deploying TAB can be done independently as long as the changes are backward-compatible with TAO/TAE.

In essence, the deployment of TAB aligns with modern cloud-native practices: **containerized, configurable via env vars, probed for health, and monitored via metrics**. This ensures reliability and ease of maintenance in production environments.

## API Endpoints and Front-end Interface

TAB exposes a set of API endpoints that allow the front-end (or other clients) to interact with the team-building process. These endpoints handle everything from starting a new build session, sending user inputs to the wizard, to retrieving suggestions or final results. They form the contract between the UI and the TAB backend. Below we enumerate the key endpoints and their roles:

* **POST /build\_sessions** – **Start a New Build Session**: This endpoint begins a new configuration session. The request body can include parameters like:

* type: one of "TEAM", "DOMAIN", "TENANT" indicating what the user wants to build.

* yaml\_config (optional): A YAML file or string content. If provided, the session will attempt YAML import first.

On success, the response will typically include a generated session\_id (which the client will use for subsequent calls) and the first prompt/question (if the wizard needs to ask something). For example:

{  
  "session\_id": "123e4567-e89b-12d3-a456-426614174000",  
  "prompt": "What is the primary goal of the agent team you want to create?"  
}

If a YAML was provided and was complete, the response might indicate that the build is already complete (and possibly include the created team ID or a message like "Configuration accepted, team created").

* **POST /build\_sessions/{session\_id}/answer** – **Submit an Answer/Response**: Once a session is in progress (likely in wizard mode), the front-end uses this endpoint to send each answer from the user. The request body might have:

* answer: The user's answer text (or a selection).

* Optionally, if the UI provides structured input (like forms for multiple fields in one step), the payload could carry those fields.

The response will contain either the next prompt to display or a flag that the configuration is complete. For example:

{  
  "prompt": "How many agents do you want in this team and what are their roles?",  
  "suggestions": \["You can say e.g. 'two agents: a researcher and an analyst'."\]  
}

or if that answer completed everything:

{  
  "status": "completed",  
  "summary": "Team 'MarketResearchTeam' has been created with 2 agents.",  
  "team\_id": "team\_42"  
}

(The exact format can vary; maybe it redirects to a GET endpoint to fetch details of the created entity.)

* **GET /build\_sessions/{session\_id}** – **Get Session Status/Context**: This can be used by the front-end to poll or retrieve the current state of a session. For instance, if the front-end reloads (user refreshes page mid-wizard), it can call this to get what question was last asked or if the session is still active. The response might include:

* status: e.g. "in\_progress"/"paused"/"completed".

* If in progress, the current\_prompt (and any partial info to display maybe).

* If completed, the result summary or references (like created IDs).

Example:

{  
  "status": "in\_progress",  
  "current\_prompt": "How many agents do you want in this team and what are their roles?",  
  "steps\_completed": 2  
}

If session is paused, status may be "paused" but current\_prompt might still be available for resume.

* **POST /build\_sessions/{session\_id}/pause** – **Pause Session**: This could explicitly mark a session as paused (though simply not continuing also pauses). This might be useful if the user clicks a "pause" button, so the backend knows the user intentionally stopped at a good point. It might respond with an acknowledgment. (This could also be done implicitly without an endpoint, but having one allows a clean cutoff and maybe triggers some cleanup or notification.)

* **POST /build\_sessions/{session\_id}/resume** – **Resume Session**: To explicitly resume a paused session. Alternatively, the workflow might resume automatically when an answer is posted to a paused session, so this endpoint might not be strictly necessary. But it could be used to fetch the next prompt after a pause, or in some designs to allocate a wizard instance again. If used, it likely just returns the next question to ask (same as what GET status might do if in\_progress).

* **POST /build\_sessions/{session\_id}/cancel** – **Cancel/Abort Session**: In case the user wants to abort the process entirely. This would mark the session as canceled (or delete it) and possibly rollback any partial database changes. (Since final commit happens at end, usually nothing is permanently created until completion, so cancel mostly just discards the session.) The API returns success acknowledgment. This helps free any resources (though in our design, not much stays allocated aside from DB rows).

* **GET /suggestions** (or maybe part of another endpoint) – **Tool Suggestions**: Depending on UI design, suggestions might come embedded in wizard responses (as text). But if the front-end wants more control (like to show a list of tools in a sidebar), it might call a dedicated endpoint. For example, GET /suggestions?query={text}\&type=tool could return a JSON list of suggested tools or teams for a given natural language query. The response would include names, descriptions, maybe an icon or ID:

* {  
    "suggestions": \[  
      { "type": "tool", "name": "WebSearchTool", "description": "Allows agents to search the web via Google." },  
      { "type": "team", "name": "WebBrowserAgentTeam", "description": "Agent team that can browse web pages and extract info." }  
    \]  
  }

* The front-end could use this if it has a “search for solutions” feature separate from the main wizard conversation. In MVP, we might not expose it separately, keeping it internal to wizard, but it's an option.

* **GET /tenants**, **GET /domains**: While not strictly part of building process, the UI might need to populate some dropdowns (like choosing a domain for a team, or listing tenants if an admin is starting a new domain and can choose which tenant to attach—though if admin is global, they'd specify tenant). Endpoints to list tenants (maybe accessible only to super-admin or the user’s own tenant info) or list existing domains could be provided. For instance, if building a team, the UI might ask which domain to use and call /domains?tenant\_id=XYZ to get options. These would query the DB.

* **GET /health/ready**, **GET /health/live** – as covered, for system use. The front-end typically wouldn’t call these (they’re for infrastructure), but listing them here for completeness.

**API Security & Authentication:** All these endpoints are protected via authentication (likely a bearer JWT or session token). The user’s token carries roles/permissions: \- The server checks, for example: \- Only tenant admins (or above) can access /build\_sessions for type TEAM or DOMAIN within their tenant. \- Only platform admins can create a tenant (if type=TENANT and user not platform admin, it returns 403 Forbidden). \- Only the user who started a session can continue that session (session carries user\_id, compare to auth token). This prevents one user from sniffing or interfering with another’s config process. \- The suggestions endpoint, if exposed, should also require appropriate rights (maybe any authenticated user, but likely only those who can build or view tools).

**Front-end Interaction Pattern:** Typically: 1\. User clicks "Create New Team" in UI \-\> UI calls POST /build\_sessions {type:"TEAM"}. 2\. UI receives session\_id and first prompt \-\> UI displays the prompt in a chat-like interface (or form). 3\. User enters answer \-\> UI calls POST /build\_sessions/{id}/answer {answer: "..."} 4\. Back-end processes, returns next prompt (and perhaps some guidance like suggestions embedded). 5\. Loop until completion \-\> backend eventually returns completed status with outcome. 6\. UI might then redirect user to a page showing the new team’s details (or refresh some list in the app).

If YAML upload: \- UI might call POST /build\_sessions {type:"TEAM", yaml\_config:file} (as multipart form if file). The backend might respond either with "completed" if file had everything, or with session\_id and a message like "partial config loaded, continuing in wizard..." along with the first question to resolve missing parts.

**Error handling:** \- If user submits an invalid answer (that fails validation), how is it returned? Possibly as part of the next prompt: e.g., next prompt might be same question rephrased with an error note. Or a separate error field in JSON like:

{ "error": "The tool name you provided is not recognized.", "prompt": "Please provide a valid tool name or description:" }

The UI can then show the error. For YAML import via API, if completely invalid, the POST /build\_sessions could directly return a 400 with error details (so UI can show “YAML errors: ...”).

**API Documentation & Self-Descriptiveness:** It's good if TAB provides an OpenAPI spec (since likely FastAPI, it can auto-generate docs). This way, front-end devs can see what endpoints expect and return. The formal documentation (like this text) complements that by explaining usage scenarios.

**Real-time vs Batch:** The design described is synchronous request/response. If the wizard needed to do something lengthy (like call an LLM that takes many seconds), the API might hold the connection until response is ready (which is okay, maybe up to some timeout). Alternatively, one could design an async approach where the client polls for answer. But given an interactive chat, synchronous is simpler: the user asks, they wait, they get reply.

**WebSocket or Streaming (not in MVP):** In the future, perhaps the wizard could stream its question if it's long or if we wanted token-level streaming from an LLM. MVP likely doesn’t need that, as questions are short. But streaming might be more relevant for an agent’s final summary. For example, if after finalizing a team, the system gave a verbose summary, streaming could be nice. If needed, an upgrade to websockets could be considered later.

**Integration with Front-end UI/UX:** \- The front-end likely presents the wizard Q\&A in a chat interface (making use of the sequential prompt/answer endpoints). \- The UI could also present forms or special controls for certain answers (like if wizard asks "upload a file for domain", the UI can show a file picker and then call an endpoint maybe POST /build\_sessions/{id}/file to send the file – which our endpoints above didn't explicitly cover, but could be extension of the answer endpoint). \- For tenant creation, the UI might not use a chat interface but a multi-step form. In that case, the front-end might choose not to go through the wizard endpoints at all and instead directly call something like POST /tenants with all data. However, to maintain consistency and central logic, it might still use the wizard behind scenes. It's possible the front-end for tenant onboarding is a simple form that internally uses the YAML path (i.e., it constructs a YAML or JSON and sends to the builder). \- Thus, the endpoints allow both interactive and direct usage; front-end can pick what UX fits best.

**Example Sequence (Team via Wizard):**

POST /build\_sessions {"type":"TEAM"}  
 \-\> returns {session\_id:1, prompt:"What's the team goal?"}  
POST /build\_sessions/1/answer {"answer": "Analyze market data to produce insights"}  
 \-\> returns {prompt:"Great. How many agents... and their roles?"}  
POST /build\_sessions/1/answer {"answer": "Two agents: one Data Collector, one Analyst"}  
 \-\> returns {prompt:"What tools should the 'Data Collector' agent use? ...", suggestions:\["WebSearchTool", "DataAPIClient"\]}  
...  
POST /build\_sessions/1/answer {"answer": "Use WebSearchTool to gather data."}  
 \-\> returns {prompt:"And what tools for the 'Analyst' agent? ...", suggestions:\[...\]}  
...  
POST /build\_sessions/1/answer {"answer": "No special tools needed, just the LLM."}  
 \-\> returns {prompt:"Any existing domain knowledge the team should use? If you have a knowledge base, provide it or say None.", suggestions:\[\]}  
POST /build\_sessions/1/answer {"answer": "Use the Financial Reports domain."}  
 \-\> returns {prompt:"Alright, confirm the configuration: Team 'X' with ... (summary). Is this correct? (yes/no)"}  
POST /build\_sessions/1/answer {"answer": "yes"}  
 \-\> returns {status:"completed", team\_id:42, message:"Team created successfully."}

The UI then maybe navigates to /teams/42 view.

This demonstrates how each endpoint is used. Citations aside, the structure aligns with typical RESTful design and is understandable by front-end or integration developers.

## Security and Permission Controls

Security is paramount in a multi-tenant platform where configuration of agents could have significant implications (access to data, running code, etc.). The TAB module implements multiple layers of security and permission control:

**Role-Based Access Control (RBAC):** The platform defines user roles with specific permissions, and TAB enforces these on its actions. For example, drawing parallel from similar AI systems[\[11\]](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced#:~:text=,AI%20agents%20within%20an%20organization): \- **Platform Admin (Superuser):** Can create new tenants and has oversight of all tenants’ configurations. They have essentially all permissions (create/edit any agent teams, domains across tenants, manage roles). \- **Tenant Admin:** A user role within a tenant that can create and manage agent teams and domain knowledge for that tenant. They might also invite other users to the tenant. Typically, only Tenant Admins (or a specialized “Agent Builder” role within the tenant) are allowed to use the Team Agent Builder for that tenant’s context. \- **Tenant Editor/Member:** (If defined) could be a role that allows editing or using agents but not creating them. For instance, a “Team Editor” might adjust some settings on existing teams but not create new ones. In many systems, creation of AI agents is restricted to admins[\[12\]](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced#:~:text=Within%20the%20AI%20agents%20,there%20are%20three%20user%20roles)[\[11\]](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced#:~:text=,AI%20agents%20within%20an%20organization), to maintain quality and control. \- **Tenant Viewer/End-user:** Cannot access the builder at all; they can only use the agents (e.g., ask questions to the agents once deployed, but not change config).

TAB checks the authenticated user’s role on every relevant endpoint: \- When calling POST /build\_sessions to start a new build, if the user is not an Admin (or designated builder role) for that tenant, the request is denied with 403 Forbidden. For tenant creation, only Platform Admin passes. \- For domain creation, similarly only tenant admin or a role with knowledge-manager privileges can do it. \- During the wizard, intermediate steps also implicitly require the same auth (since the user’s token is needed for each answer post). If the user’s role changes mid-session (unlikely but conceptually), the next call would fail if they lost permission.

**Tenant Isolation:** All data being created is tagged with a tenant\_id. The back-end will ensure that users can only act within their tenant. For example, even if a malicious user tried to craft a request to build a team under a different tenant\_id, the server would ignore that field and use the tenant from the user’s auth context. Or explicitly check that the user’s tenant matches the requested tenant context. \- If a global admin is creating a domain for another tenant (maybe possible if platform supports managed service), they could specify a tenant in the request, but only because they have global rights. \- Otherwise, tenant\_id in requests is either derived from user or must match user’s tenant.

**Data Access Control:** Once teams and domains are created, their records contain tenant\_id. Any API call to fetch or list them will filter by the user’s tenant, so one tenant cannot see another’s configurations. This is typically enforced at query time, e.g., SELECT \* FROM teams WHERE tenant\_id \= current\_user.tenant\_id.

**Submission and Activation:** There might be an additional approval step for agent teams in some setups: for example, even if a user can build a team, perhaps it doesn’t go live until a higher-up approves. In MVP, we assume creation by an authorized person means it’s good to go. But as a security measure, some platforms require a review of agent configurations (for compliance or cost reasons). If that was desired, TAB could mark a new team as “pending approval” instead of active, and an admin would review and flip a flag. However, since not specified, we consider that outside MVP.

**Input Validation & Sanitization:** \- **YAML Sanitization:** As noted, we use safe YAML parsing to avoid executing any payload. We treat all inputs as data. Also, strings provided by users are sanitized to prevent injection attacks. E.g., if any input gets used in a shell command or DB query, it’s parameterized. In our case, most inputs just go into DB or LLM prompts; in DB, we use parameterized queries or ORMs to avoid SQL injection. \- **LLM Prompt Safety:** The wizard’s prompts incorporate user input. We have to ensure a user can’t manipulate the LLM into doing something undesirable (prompt injection). For example, if a user’s answer includes something like “Ignore previous instructions”, the LLM (WizardEngine) might potentially deviate. To mitigate: \- The WizardEngine can use system prompts or few-shot examples that reinforce it should stick to the script. \- Possibly filter user input for obviously malicious content or instructions. \- Since the wizard is mostly asking for factual configuration, the risk is lower than a general chatbot, but still present. We explicitly instruct the LLM (if we have such control) to only output relevant next questions and not reveal system info. \- **Tool Suggestion Filtering:** We touched on ensuring suggestions do not expose unauthorized info. If one tenant has a private custom tool, that should not be suggested to another tenant’s builder. The metadata filtering in Qdrant queries ensures suggestions are tenant-specific or globally allowed only[\[6\]](https://qdrant.tech/ai-agents/#:~:text=Multi,Multitenancy). Also, if a tool has sensitive details, the description stored should be generic enough or sanitized.

**Operational Security:** \- **Authentication**: likely uses a JWT or session from a single sign-on. We ensure that all API calls require a valid token. This includes the metrics endpoint possibly being open (which is fine if it’s internal) but user endpoints definitely locked down. \- **Encryption**: Communications to TAB should be over HTTPS in production. The DB connections might use TLS if available. If any secrets are part of config (like the YAML might include API keys for tools?), those should be stored encrypted. For example, if the wizard asks for an API key (say for Google API in a tool), the answer should be treated as secret – possibly stored encrypted in the database or in a vault, rather than plaintext. At least mark it and restrict access (maybe that goes beyond MVP; MVP could store in JSON but in future, integrate HashiCorp Vault or similar). \- **Penetration Resistance**: Basic hardening like using latest dependencies to avoid vulnerabilities, limiting container privileges (no need for root user in container), etc., are considered. \- **Rate limiting**: Not explicitly needed if only admins use it occasionally, but if many users had access, one might add rate limits to prevent spamming the LLM or DB with requests.

**Permissions Summary Example:** In a format similar to Zendesk roles[\[12\]](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced#:~:text=Within%20the%20AI%20agents%20,there%20are%20three%20user%20roles): \- Platform Admin: ✔️ Create Tenant, ✔️ Create Team (any tenant), ✔️ Create Domain (any tenant), ✔️ Manage All. \- Tenant Admin: ❌ Create Tenant, ✔️ Create Team (own tenant), ✔️ Create Domain (own tenant), ✔️ Manage users (maybe inviting). \- Tenant Editor: ❌ Create Tenant, maybe ✔️ Edit existing Teams (own tenant) but ❌ Create new (depending on design). \- Tenant Viewer: ❌ any creation or edit, only view or use.

The system might not have all these roles explicitly defined, but it’s good to note we follow the principle of least privilege: only those designated can perform configuration actions. This is in line with common AI agent platforms where only certain users can create or modify agents[\[12\]](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced#:~:text=Within%20the%20AI%20agents%20,there%20are%20three%20user%20roles).

**Audit Logging:** As part of security, all creation events are logged. Build\_steps provides a detailed log (which user input what), and we could also have a higher-level audit: “User X (ID) created Team Y in Tenant Z at time T.” This might be logged to a separate audit log or can be derived from the data by queries. In case of any security incident, one can trace who did what.

**Approval Workflow (Future Consideration):** As mentioned, might introduce something like requiring another admin to approve a newly built team. If that were to be implemented, TAB might mark the team as inactive on creation, and send an email to an approver. This wasn’t asked for MVP, but we mention as a roadmap idea perhaps.

By enforcing strict role checks and isolating tenant data, TAB ensures that only authorized users can build or modify agent teams, and that one tenant cannot interfere with another. This prevents accidental or malicious misuse of the builder. For instance, without these controls, a normal user might spin up an agent team connected to sensitive internal tools or cause chaos; RBAC prevents that by limiting the builder to trusted roles[\[11\]](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced#:~:text=,AI%20agents%20within%20an%20organization).

## MVP Scope vs. Future Enhancements

Finally, it’s important to delineate which features are part of the Minimum Viable Product (MVP) release of TAB and which are slated for future phases. This helps set expectations and provide a roadmap for development.

**MVP Features (Phase 1):** \- **Conversational Team Builder Wizard:** Fully functioning for agent team creation with basic question flow (team name, goal, agent roles, tool selection from known tools, domain selection). Utilizes LangGraph for structured flow, and supports pause/resume for long sessions. \- **YAML Import (Basic):** Ability to upload a YAML defining agents, with schema validation. Fallback to wizard for missing fields or minor corrections. MVP might assume the YAML format is exactly as documented, without too much flexibility. (I.e., a strict schema to follow.) \- **Domain and Tenant Creation via Wizard:** Included, but likely straightforward. Domain creation in MVP might allow adding textual info or a simple file upload for knowledge (but possibly not full automated ingestion yet – maybe it just stores the file reference). Tenant creation in MVP collects basic info and writes to DB, without any advanced customization beyond that. \- **Semantic Tool Suggestions (Basic):** The Qdrant-based suggestion engine is implemented with an initial catalog of tools. MVP likely uses a fixed embedding model (e.g., OpenAI text-embedding-ada) and has populated a few dozen tool descriptions. It provides suggestions in the wizard, but might not have the team-as-tool fully functional (could suggest teams conceptually, but integrating a team as a tool might not be automatic in MVP). \- **Postgres Data Storage:** Use Postgres for sessions and final config data. Possibly all final team/agent records are also in Postgres (and maybe we skip Neo4j for MVP to reduce complexity, or only use it read-only for visualization). The schema defined is implemented. \- **Health checks & Metrics:** Basic health endpoints working. Prometheus metrics for HTTP requests enabled[\[8\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=following%3A%20First%2C%20let%E2%80%99s%20see%20http%3A%2F%2Flocalhost%3A5000%2F,query%20bar%20and%20running%20Execute). Custom metrics might be minimal in MVP (maybe just request metrics and a counter of sessions started). \- **Docker Deployment:** The service can run in Docker and has been tested in a docker-compose with a local Postgres and Qdrant. Documentation (like this) is provided for how to configure env variables. \- **Security & Roles:** Integration with the platform’s auth (likely JWT with roles in claims). The endpoints check roles as described. MVP might define roles in a config (since user management might be external). \- **Basic Front-end Integration:** There is likely a simple UI that calls these endpoints (maybe a reference implementation or the actual product UI). The flows have been tested end-to-end manually.

What MVP may **not** include (but planned later): \- **Rich UI/UX features:** For instance, the wizard conversation in MVP might be text-based only. Future could add GUI elements (like selecting tools from a list visually, or dragging agent icons). The current system might rely on user typing things or clicking yes/no, etc., in a textual chat. \- **Advanced Error Handling:** MVP handles expected validation but might not cover every edge gracefully. Future enhancements will add more user-friendly error prompts (like suggesting corrections). \- **Complex Agent Config Options:** In MVP, the agent definitions might be simple (name, role, tools, model). Future phases may allow specifying detailed prompt templates for agents, memory settings, or chain logic. Those would require more complex wizard questions or maybe a different interface (like an advanced mode). \- **Team-as-Tool Implementation:** While suggested, MVP might not allow an agent team to directly call another as a tool unless manually configured by a developer. A future phase could formalize this (like the builder could link an existing team into a new team config with a wrapper agent). \- **Automated Domain Ingestion:** MVP possibly just records domain info. A future phase might connect to a pipeline that automatically reads the documents, generates embeddings, and stores them for use by agents. At that point, the builder wizard might even show progress of indexing or allow verifying extracted info. \- **Collaboration in Building:** MVP assumes one user building at a time. Future could allow multiple users to contribute (though that’s unusual for such a config, but maybe a review/comment system). \- **Multi-language support:** MVP likely in English only. Future might allow the wizard to operate in other languages or output agent prompts in localized language. \- **Performance Optimizations:** MVP is functional but might not be heavily optimized. Future might bring caching of suggestion results, asynchronous processing to keep UI snappy (e.g., pre-fetch suggestions while user is typing), or scaling out the LLM calls with higher concurrency if needed. \- **Testing & Quality Improvements:** MVP will have unit/integration tests for core, but future phases will add more extensive testing, including security audits and maybe adversarial testing (ensuring prompt injection doesn’t cause issues, etc.). \- **Continuous Improvement via Logs:** In future, data from build\_steps could be analyzed to see where users struggle, and wizard questions could be adjusted. Possibly integration with LangChain’s LangSmith or similar for agent observability (LangGraph has integration to trace flows[\[13\]](https://blog.langchain.com/building-langgraph/#:~:text=,Platform%20to%20answer%20this%20need)). \- **UI for Audit/Editing:** Post-MVP, an admin interface might be built to view a team config created and tweak it manually (maybe editing the YAML or a form). Right now, after creation, any changes would require starting over or editing YAML directly in DB. Future might allow an “edit team” feature that reopens a wizard with pre-filled answers (that would be a neat extension – load an existing config into the wizard for modification).

In summary, the MVP delivers the core promise: the ability to create multi-agent teams (and associated configs) through an intuitive wizard or via config files, storing the results and integrating with the rest of the system (TAO/TAE)[\[14\]](https://qdrant.tech/ai-agents/#:~:text=LangGraph%20Framework%20for%20managing%20multi,structured%20graphs%20for%20AI%20agents). Subsequent phases will iterate on user experience, advanced capabilities, and integration depth (like fully leveraging domain knowledge, or expanding the library of available agent skills).

---

*This documentation has provided an in-depth look at the Team Agent Builder (TAB) module – from its high-level role in the platform down to its submodule design, data schemas, and interfaces. By following the structure and principles outlined (and aligning with the established TAO module’s patterns), TAB is set up to be a robust, extensible component of the Team Agent Platform. Its conversational approach lowers the technical barrier, while its integration points ensure that the configurations it produces are immediately useful and manageable in production. With proper deployment, security controls, and a clear roadmap for enhancements, TAB will serve as the foundation for scaling up the creation of complex AI agent ecosystems in a controlled and user-friendly manner.*[\[3\]](https://medium.com/the-savvy-canary/avoiding-multi-agent-system-complexity-with-yaml-f4f958610930#:~:text=The%20YAML%20way)[\[5\]](https://qdrant.tech/ai-agents/#:~:text=Contextual%20Precision%20Qdrant%E2%80%99s%20hybrid%20search,Hybrid%20Search)

---

[\[1\]](https://blog.langchain.com/building-langgraph/#:~:text=%2A%20Human,the%20only%20way%20to%20go) [\[4\]](https://blog.langchain.com/building-langgraph/#:~:text=want%20to%20save%20checkpoints%20that,user%20or%20developer%20for%20input) [\[13\]](https://blog.langchain.com/building-langgraph/#:~:text=,Platform%20to%20answer%20this%20need) Building LangGraph: Designing an Agent Runtime from first principles

[https://blog.langchain.com/building-langgraph/](https://blog.langchain.com/building-langgraph/)

[\[2\]](https://qdrant.tech/ai-agents/#:~:text=Contextual%20Precision%20Qdrant%E2%80%99s%20hybrid%20search,based%20strategies) [\[5\]](https://qdrant.tech/ai-agents/#:~:text=Contextual%20Precision%20Qdrant%E2%80%99s%20hybrid%20search,Hybrid%20Search) [\[6\]](https://qdrant.tech/ai-agents/#:~:text=Multi,Multitenancy) [\[14\]](https://qdrant.tech/ai-agents/#:~:text=LangGraph%20Framework%20for%20managing%20multi,structured%20graphs%20for%20AI%20agents) AI Agents with Qdrant \- Qdrant

[https://qdrant.tech/ai-agents/](https://qdrant.tech/ai-agents/)

[\[3\]](https://medium.com/the-savvy-canary/avoiding-multi-agent-system-complexity-with-yaml-f4f958610930#:~:text=The%20YAML%20way) Avoiding Multi-agent System Complexity with YAML | by Gene Schank | The Savvy Canary | Sep, 2025 | Medium

[https://medium.com/the-savvy-canary/avoiding-multi-agent-system-complexity-with-yaml-f4f958610930](https://medium.com/the-savvy-canary/avoiding-multi-agent-system-complexity-with-yaml-f4f958610930)

[\[7\]](https://dev.to/lisan_al_gaib/building-a-health-check-microservice-with-fastapi-26jo#:~:text=What%20This%20Project%20Covers) Building a Health-Check Microservice with FastAPI \- DEV Community

[https://dev.to/lisan\_al\_gaib/building-a-health-check-microservice-with-fastapi-26jo](https://dev.to/lisan_al_gaib/building-a-health-check-microservice-with-fastapi-26jo)

[\[8\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=following%3A%20First%2C%20let%E2%80%99s%20see%20http%3A%2F%2Flocalhost%3A5000%2F,query%20bar%20and%20running%20Execute) [\[9\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=from%20prometheus_client%20import%20Counter) [\[10\]](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e#:~:text=REQUEST_COUNT%20%3D%20Counter%28%20,) Prometheus on a FastAPI application | by Hitoruna | Jul, 2025 | Medium

[https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e](https://medium.com/@hitorunajp/prometheus-on-a-fastapi-application-aa25e5223a9e)

[\[11\]](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced#:~:text=,AI%20agents%20within%20an%20organization) [\[12\]](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced#:~:text=Within%20the%20AI%20agents%20,there%20are%20three%20user%20roles) Understanding user roles for AI agents \- Advanced – Zendesk help

[https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced](https://support.zendesk.com/hc/en-us/articles/8357756905626-Understanding-user-roles-for-AI-agents-Advanced)