version: '3.8'

services:
  # ============ DATABASES (CPU-Optimized) ============
  postgres:
    image: postgres:16-alpine
    container_name: ta_v8_postgres
    environment:
      POSTGRES_DB: ta_v8
      POSTGRES_USER: postgres_user
      POSTGRES_PASSWORD: postgres_pass
      PGDATA: /var/lib/postgresql/data/pgdata
      # Performance tuning for 64GB RAM
      POSTGRES_MAX_CONNECTIONS: 200
      POSTGRES_SHARED_BUFFERS: 4GB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 16GB
      POSTGRES_MAINTENANCE_WORK_MEM: 1GB
      POSTGRES_WAL_BUFFERS: 16MB
      POSTGRES_RANDOM_PAGE_COST: 1.1
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./backups:/backups
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres_user -d ta_v8"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============ VECTOR DATABASE (CPU-Optimized) ============
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./data/qdrant:/qdrant/storage:z
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__SERVICE__HOST: 0.0.0.0
      QDRANT__STORAGE__STORAGE_PATH: /qdrant/storage
      # Performance optimizations
      QDRANT__STORAGE__WAL__WAL_CAPACITY_MB: 1000
      QDRANT__STORAGE__OPTIMIZERS__MEMMAP_THRESHOLD_KB: 100000
      QDRANT__SERVICE__MAX_REQUEST_SIZE_MB: 256
      QDRANT__SERVICE__GRPC_MAX_RECEIVE_MESSAGE_LENGTH: 268435456
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
    # Fixed health check - use root endpoint instead of /health
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ============ GRAPH DATABASE (Memory-Optimized) ============
  neo4j:
    build:
      context: ./neo4j
      dockerfile: Dockerfile.neo4j
    container_name: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - ./data/neo4j:/data
      - ./data/neo4j/logs:/logs
      - ./neo4j/import:/var/lib/neo4j/import
      - ./neo4j/plugins:/plugins
    environment:
      NEO4J_AUTH: neo4j/pJnssz3khcLtn6T
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_apoc_export_file_enabled: "true"
      NEO4J_apoc_import_file_enabled: "true"
      NEO4J_apoc_import_file_use__neo4j__config: "true"
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*"
      NEO4J_dbms_security_procedures_allowlist: "apoc.*"
      # Fixed memory settings for Neo4j 5.15
      NEO4J_server_memory_heap_initial__size: 4G
      NEO4J_server_memory_heap_max__size: 8G
      NEO4J_server_memory_pagecache_size: 8G
      # Remove invalid settings
      # NEO4J_db_tx__state_memory__allocation: on_heap  # REMOVED - Not valid
      # NEO4J_db_tx_state_memory_max_off_heap_memory: 2G # REMOVED - Not valid
      # Query cache
      NEO4J_dbms_query__cache__size: 100
      # Disable strict validation to avoid startup errors
      NEO4J_server_config_strict__validation_enabled: "false"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 20G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7474"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ============ OBJECT STORAGE (IO-Optimized) ============
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_BROWSER_REDIRECT_URL: http://localhost:9001
      # Performance settings
      MINIO_API_REQUESTS_DEADLINE: 10m
      MINIO_API_REQUESTS_MAX: 10000
    volumes:
      - ./data/minio:/data
    command: server --console-address ":9001" /data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # ============ LLM SERVICE (GPU-Optimized with 70B Model Support) ============
  # ============ LLM SERVICE (GPU-Optimized for GPT-OSS 20B) ============
  ta_v8_ollama:
    container_name: ta_v8_ollama
    image: ollama/ollama:0.12.0
    restart: unless-stopped
    ports: 
      - "11434:11434"
    environment:
      # Optimized for RTX 5090 - maximize performance while maintaining stability
      OLLAMA_MODELS: /root/.ollama/models
      OLLAMA_MAX_LOADED_MODELS: "2"          # Can handle 2 models with 45% GPU free
      OLLAMA_NUM_PARALLEL: "2"              # Enable 2 concurrent requests
      OLLAMA_KEEP_ALIVE: 30m                 # Keep model loaded longer for better UX

      # Optimized context: 16K tokens (up from 8K) - using available VRAM
      OLLAMA_CONTEXT_LENGTH: "16384"

      # RTX 5090 optimizations
      OLLAMA_FLASH_ATTENTION: "1"           # Essential for memory efficiency
      OLLAMA_GPU_LAYERS: "-1"               # Load all layers on GPU
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_DEBUG: INFO

      # Default model for auto-loading
      OLLAMA_MODEL: gpt-oss:20b
      
      # GPU settings optimized for RTX 5090
      CUDA_VISIBLE_DEVICES: "0"
      OLLAMA_GPU_OVERHEAD: "2048"           # 2GB overhead for safety
    volumes:
      - ./models/ollama:/root/.ollama
      - ./scripts/ollama-init.sh:/scripts/ollama-init.sh:ro
    entrypoint: ["/bin/sh", "-c"]
    command: ["/scripts/ollama-init.sh & /bin/ollama serve"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: ["gpu"]
        limits:
          memory: 56G   # Optimized: 88% of 64GB RAM (8GB reserved for system)
          cpus: '16'    # Use 16 of 24 cores for better parallel processing
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/version"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 180s  # Longer startup time for large models

  # ============ EMBEDDING SERVICE (GPU-Optimized) ============
  multilingual-e5-large:
    build:
      context: ./multilingual-e5-large
      dockerfile: Dockerfile.multilingual-e5-large
    container_name: multilingual-e5-large
    ports:
      - "8080:8080"
    environment:
      OMP_NUM_THREADS: 4
      TOKENIZERS_PARALLELISM: "false"
      TRANSFORMERS_OFFLINE: "0"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 16G
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

networks:
  default:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 1500
